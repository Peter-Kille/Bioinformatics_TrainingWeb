[
  {
    "objectID": "0.1_Personal_Cloud.html",
    "href": "0.1_Personal_Cloud.html",
    "title": "Personal Cloud",
    "section": "",
    "text": "High performance computing (HPC) systems are large, multi-processor, large RAM ‘computers’ that are installed in data centres. Usually users are provided with a login to a shared HPC system with strictly controlled access and defined quotas. To run your informatic tasks, you write a script that sends your job to a ‘queuing’ program which coordinates the processing requests of all the users on the system. In this way the system can support large numbers of users, but you need to wait for your place in the queue before your task can be performed. Although you can do the very lightest tasks - copy small files, rename etc. - without sending them to the queue, most tasks need to go through the queue. This can be very tedious when learning to code, as if your script creates an error you won’t know until it has waited its turn in the queue.\nTo avoid the queues we provide personal cloud systems to each person on the course, allowing them to interactively use the HPC system. To do this we create a container which has the equivalent of a ‘virtual machine’ (VM) in the cloud; we use a system called Kubernetes to create these container-based virtual machines. Each user has dedicated processors, RAM, storage and their own environment so they can interactively learn to use a Linux system. Because we are having to reserve a proportion of the server for each user we can only allocate a limited number of processors and RAM to each user - usually this is 8 processors and 16 Gb RAM, but check with your course organiser.\nSimilar ‘containers’ can be accessed using AWS cloud services or through academic services such as CLIMB. AWS cloud does provide some educational free cloud processing but when these free credits are over you would need to pay for processing and storage, and this can very quickly get expensive. CLIMB provides academic registration if you are supported by specific research council funding.\nThis information below is aimed at students and staff in Cardiff University’s School of Biosciences, and will guide you through accessing the computing resources provided."
  },
  {
    "objectID": "0.1_Personal_Cloud.html#ssh-using-mobaxterm---connecting-to-the-server-on-a-pc",
    "href": "0.1_Personal_Cloud.html#ssh-using-mobaxterm---connecting-to-the-server-on-a-pc",
    "title": "Personal Cloud",
    "section": "SSH using MobaXterm - connecting to the server on a PC",
    "text": "SSH using MobaXterm - connecting to the server on a PC\nMobaXterm is a free piece of software called an SSH client, which enables you to create a SSH connection between your PC and the server. It has a built-in SFTP (Secure File Transfer Protocol) for file transfer, X-forwarding (graphical emulation) for using graphical applications remotely, and allows you to open multiple windows to your server. You can download the free version of MobaXTerm from their website.\n\n\n\n\n\n\nUsing MobaXterm on your personal PC\n\n\n\n\n\nI suggest downloading and installing the ‘Installer Addition’ as it will fully install on your system - the portable version provide a simple executable (.exe) that can be placed on a data pen or used in machines where you do not have administrator privileges to install software.\n\n\n\n\n\n\n\n\n\nUsing MobaXterm on a University-networked PC\n\n\n\n\n\nYou will not have administrator privileges to install the ‘Installer Addition’ on a University-managed machine. Instead, download the portable version which provides a simple executable (.exe) that can used on a machine where you do not have administrator privileges to install software.\n\n\n\nOnce your download is complete, you will need to unzip the folder (extract files) before you can run the installer. Then follow the installation instructions and you should be faced with the follow window:\n\n\n\nMobaXterm Interface\n\n\nN.B. The first time you open MobaXterm, Windows security may pop up saying that some features are blocked by the firewall. Click ‘Cancel’ and the message will go away without preventing MobaXterm from doing its thing.\nNow you want to create a session, this can be done following these easy steps:\n\nSelect Session icon\n\n\n\n\nMobaXterm session\n\n\n\nSelect SSH icon\n\n\n\n\nMobaXterm SSH\n\n\n\nConfigure your SSH connection by filling in boxes for -Host -User and -Port\n\nN.B. - the three elements you need to complete are underlined in red in the image.\nN.B. - for standard HPC the port leave the port at the default value of 22\n\n\n\nMobaXterm Configure\n\n\n\nEnter your passphase\n\nYou will NOT see any typing - this is a security feature, just continue typing\n\n\n\nMobaXterm Password\n\n\nOnce you have entered your password and it has been accepted you MAY be asked to save a master password - this you can set to whatever you wish, if you want to save yourself the trouble of typing your pass phrase every time you connect.\nOnce created, your ‘session’ will be saved in the left hand menu and in future you just have to click on it to connect."
  },
  {
    "objectID": "0.1_Personal_Cloud.html#ssh-using-a-terminal---connecting-to-the-server-on-a-mac",
    "href": "0.1_Personal_Cloud.html#ssh-using-a-terminal---connecting-to-the-server-on-a-mac",
    "title": "Personal Cloud",
    "section": "SSH using a terminal - connecting to the server on a Mac",
    "text": "SSH using a terminal - connecting to the server on a Mac\nHPC servers - both your personal cloud as well as HPC systems - are based on a Unix Operating System called Linux. Mac OS is also based on this system, so connecting on a Mac is very simple. First you have to find and open your Terminal - I think the easiest way is to search for it in finder but if you don’t find this easy there’s a wiki entitled - Open-a-Terminal-Window-in-Mac.\nOnce you have your terminal open simply type:\nssh [user]@[host] -P [Port]\n\n#a fictious example would be\n\nssh c99999@sponsa.bios.cf.ac.uk -P 32222\nFor standard HPC (i.e. not Kubernetes) there is no need to define the port with -P\nThen enter your password and you are in - you can tell it has worked because the text preceding your cursor changes and should look something like (the numbers will probably be different for you):\n[user@ssh-user-894cfb776-88kzg ~]$\nUnfortunately the standard SSH interface does not support X-forwarding for graphical applications. There is a mac application called (Xquartz)[https://www.xquartz.org/] that you can install, and when it is running you can create a X-11 session by simply adding -X to your login so your login would be:\nssh -X [user]@[host] -P [Port]\n\n#a fictious example work be\n\nssh -X c99999@sponsa.bios.cf.ac.uk -P 32222\nDon’t worry if you don’t understand the bit about Xquartz - it’s not essential, just know that it’s here for future reference if needed. Xquartz is open source software and Mac DO NOT SUPPORT it, so it can be challenging to get working. Newer Macs often do not support it as the open source developers haven’t kept up with changes in the OS. Play if you want too, but be warned that it can be frustrating."
  },
  {
    "objectID": "0.1_Personal_Cloud.html#sftp-using-mobaxterm-on-a-pc",
    "href": "0.1_Personal_Cloud.html#sftp-using-mobaxterm-on-a-pc",
    "title": "Personal Cloud",
    "section": "SFTP using MobaXterm on a PC",
    "text": "SFTP using MobaXterm on a PC\nMobaXterm has a built in SFTP function. When you open your session, a folder structure appears down the left hand side of the screen. This can be useful for navigating and for opening files without the need to download them.\nUse the arrows above the SFTP windows to upload/download files - you can also drag and drop files using this. window.\n\n\n\nMobaXterm SFTP"
  },
  {
    "objectID": "0.1_Personal_Cloud.html#sftp-using-filezilla",
    "href": "0.1_Personal_Cloud.html#sftp-using-filezilla",
    "title": "Personal Cloud",
    "section": "SFTP using FileZilla",
    "text": "SFTP using FileZilla\n(FileZilla)[https://filezilla-project.org/] is a open source program that works on all platforms and supports multiple upload and download options including SFTP.\nTo configure FileZilla follow the steps below and this will create a saved profile you can go back to:\n\nOpen and select profile icon\n\n\n\n\nFilezilla profile\n\n\n\nConfigure your SFTP profile\n\nRemember for standard HPC the SFTP port will default to 22; if you are working on a Kubernetes virtual machine you will need to change it.\n\n\n\nFilezilla profile\n\n\nSelect Connect and you are done - you can return to this session next time you want to transfer\n\nDrag and drop your files from local to server (remote host) system\n\n\n\n\nFilezilla drag and drop"
  },
  {
    "objectID": "0.1_Personal_Cloud.html#sftp-using-cyberduck",
    "href": "0.1_Personal_Cloud.html#sftp-using-cyberduck",
    "title": "Personal Cloud",
    "section": "SFTP using Cyberduck",
    "text": "SFTP using Cyberduck\n[Cyberduck][https://cyberduck.io/download/] is another free file transfer application that supports all type of file transfer including SFTP, and some people prefer it to FileZilla as it has a very simple interface.\nAfter downloading and installing the application open the transfer window by using the following steps:\n\nOpen a Cyberduck connection\n\n\n\n\nCyberduck Open a Connection\n\n\n\nConfigure the connection - complete all the things underlined here.\n\nUse your port number provided for Kubernetes, or if using standard HPC use port 22.\n\n\n\nCyberduck configuration\n\n\n\nYou can now navigate around the server and drag and drop files to and from server.\n\n\n\n\nCyberduck drag and drop window"
  },
  {
    "objectID": "1.1_Introduction to Linux.html#useful-information",
    "href": "1.1_Introduction to Linux.html#useful-information",
    "title": "Introduction to Linux",
    "section": "Useful information!",
    "text": "Useful information!\n• You don’t need to type the $ at the beginning of a command. That’s a linux convention to indicate a command line\n• Always tab-complete! Pressing tab will auto-complete a file name or program. If you’re writing a file named myFirstSequencingRun.fastq (note the lack of spaces!) and type just ‘myF’ and press tab, it will complete it for you, which helps on typos. If there are multiple options with the same beginning then pressing tab twice will show the options. Most Linux errors early on are because of typos or pointing to the wrong folder! Tab complete stops this, and if it won’t tab-complete then it doesn’t exist!\n• If you still have a ‘file not found’ error, do ls to see if you can see it.\n• If you can’t see a file you expect, do pwd and check you’re in the right folder.\n• If a script says “permission denied” make sure:\n\nIt has executable permission.\nYou’re not inside the classdata folder!\n\n• You always want to be working in the Working Directory. To get there (assuming you’re on a Cardiff virtual machine) use:\n\ncd ~/mydata\n\n• Once more, tab-complete is your friend! It’ll stop most typos from happening and save you the pain of writing a command for a file that’s not there. It’s worth re-iterating for the amount of time and pain it will save you."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#anatomy-of-a-command",
    "href": "1.1_Introduction to Linux.html#anatomy-of-a-command",
    "title": "Introduction to Linux",
    "section": "Anatomy of a Command",
    "text": "Anatomy of a Command\nLinux/Unix commands usually take the form shown below\n\ncommand       parameters      arguments\n   ^              ^               ^\nwhat I         how I want     on what do\nwant to do     to do it       I want to do it\n\nN.B. usually each element is separated by only one space\nThe first item you supply on the command line is interpreted by the system as a command – something the system should do. Any options for the command, such what file you want the command to work on, or the format for the information that should be returned to you, appear after that on the same line separated by spaces.\nMost commands have options available that will alter the way they function. You make use of these options by providing the command with parameters (sometimes called flags), some of which will take arguments.\nReminder: Items on the command line separated by spaces are interpreted as individual pieces of information for the system. For this reason, a filename with a space in it will be interpreted as two filenames, as will some symbols. This is important!\n\nLearning about Linux commands\nMost Linux commands have a manual page that provides information about the command and options that can alter its behaviour. Many tasks can be made easier by using command options. Linux manual pages are referred to as man pages. To open the man page for a particular command, you just need to type man followed by the name of the command you are interested in. To browse through a man page, use the cursor keys (↓ and ↑). To close the man page simply hit the q key on your keyboard.\n\n\n\n\n\n\nExercise: looking up a man page\n\n\n\nLook up the manual information for the ls command by typing the following in a terminal:\n\nman ls\n\n\nSkim through the man page. You can scroll forward using the up and down arrow keys on your keyboard. You can go forward a page by using the space bar, and move backwards a page by using the b key.\nWhat does the -m option do? What about the -a option? What would running ls -lrt do?\nPress the q key when you want to quit reading the man page.\nTry running ls using some of the options mentioned above.\n\n\n\nNote: Programs (rather than core linux commands) often have help pages that can be accessed in the same manner as a linux command manual but using -h or -help (often both will work). Others will default to show you the help page if you run the program with no arguments or have a typo."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#changing-and-making-directories",
    "href": "1.1_Introduction to Linux.html#changing-and-making-directories",
    "title": "Introduction to Linux",
    "section": "Changing and making directories",
    "text": "Changing and making directories\nThe command used to change directories is cd\nIf you think of your directory structure, (i.e. this set of nested file folders you are in), as a tree structure, then the simplest directory change you can do is move into a directory directly above or below the one you are in.\nTo change to a directory one below you are in, just use the cd command followed by the subdirectory name i.e., fi you have just logged in and want to move into the ‘mydata’ directory (inside your current directory), you could use:\n\ncd mydata\n\nThe shortcut for “the directory you are currently in” is a single full stop ( . ). If you type\n\ncd .  \n\nnothing will change. Later, when we want to run scripts or copy to the same folder that you are currently in we will use ” ./ ” to mean “you can find this in the current directory”. To change directory to the one above you are in, use the shorthand for “the directory above” which is two full stops:\n\ncd ..\n\nFor example, if you are in ‘mydata’, this will move you up one level. If you need to change directory to one far away on the system, you could explicitly state the full path. This always starts with a forward slash, because a forward slash indicates the very top of the file tree:\n\ncd /usr/local/bin\n\nIf you wish to return to your home directory at any time, just type cd by itself.\n\ncd\n\nTo make a new directory, use the command mkdir and then the directory you want to create\n\nmkdir mydata/brilliantIdeas\nmkdir ~/mydata/brilliantIdeas/InProgress\n\nIf you get lost and want to confirm where you are in the directory structure, use the pwd command (print working directory). This will return the full path of the directory you are currently in.\nNote also that by default you see the name of the current directory you are working in as part of your prompt.\nFor example, when you first opened the terminal in a live session you should see your username and server, then the prompt:\n\n[sbi9srj@hawker ~]$\n\nThis means I am logged in as the user sbi9srj on the server named hawker, and in a directory named the character called ‘tilde’ ~. If you remember, ~ is always a shortcut for your home directory.\nAll our folders and classdata can be found at:\n\n/home/[your username]/classdata/\n\nHowever, we won’t need to remember that as we can use the shortcuts of:\n\n~/classdata\n~/mydata\n\n\n\n\n\n\n\nExercises: changing directories\n\n\n\n\nChange directory from your home directory to your working directory (mydata)\nFind the full path to the directory that you are currently in\nCreate a new directory inside mydata\nMove into your new directory.\nMove back into mydata."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#moving-and-copying-data",
    "href": "1.1_Introduction to Linux.html#moving-and-copying-data",
    "title": "Introduction to Linux",
    "section": "Moving and copying data",
    "text": "Moving and copying data\nA standard format is used to move and copy data:\ncommand source destination\nFor example, to move a file named RNAseq_1.fastq into a new folder that exists in this directory named exp1:\n\nmv RNAseq_1.fastq exp1\n\nTo move it to a location somewhere else on the server you can use the full path\n\nmv RNAseq_1.fastq ~/mydata/experiment1\n\nOr alternatively “move” a file from one name to another. This is a common way to rename your files:\n\nmv RNAseq_1.fastq LRubellus_1.fastq\n\nTo copy, use the cp command. You could copy a file from another directory to “here”, remembering that a full-stop means “the folder I’m currently in”:\n\ncp ~/classdata/RNAseq_1.fastq  .\n\nOften you’ll want to copy a whole folder (directory), and you will need the -r parameter for “recursive”.\n\ncp -r mydata/all_fastqs NewExperiment/testdata/\n\n\n\n\n\n\n\nExercises: copying files and folders\n\n\n\n\nUse ls to look in the folder named\n\n\n~/classdata\n\nand\n\n~/classdata/Session1\n\n\nCopy the file CopyExercise.txt into mydata\nCopy the classdata folder named ‘Session1’ to your mydata folder [IMPORTANT! You need this for the following exercises!]\nDo ls on mydata. What do you see?"
  },
  {
    "objectID": "1.1_Introduction to Linux.html#listing-files-in-a-directory",
    "href": "1.1_Introduction to Linux.html#listing-files-in-a-directory",
    "title": "Introduction to Linux",
    "section": "Listing files in a directory",
    "text": "Listing files in a directory\nThe command ls lists files in a directory. By default, the command will list the filenames of the files in your current working directory.\nIf you add a space followed by a –l (that is, a hyphen and a small letter L), after the ls command, it alters the behavior of the command: it will now list the files in your current directory, but with details about them including who owns them, what the size is, and what kind of file it is.\nYou can also use glob patterns to limit the files you wish to list.\n\n\\* an asterisk means any string of characters\n? a question mark means a single character\n\\[ \\] square brackets can be used to designate a group of characters\n\ne.g. to list all of blue.txt, banana.txt, baby.txt:\n\nls b*.txt\n\n\n\n\n\n\n\nExercises: listing files\n\n\n\n\nList all the files in the directory Session1 that start with the letters sub\nList all the files in the directory Session1 that start with the letters sub\nList all the files in your directory that start with sub, and end in fasta\n[Extension] Do a ‘long list’ (-l) of these files and see the difference in sizes. Add the ‘human readable’ parameter/flag (-h) to help read the sizes."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#file-permisions",
    "href": "1.1_Introduction to Linux.html#file-permisions",
    "title": "Introduction to Linux",
    "section": "File permisions",
    "text": "File permisions\nThe command ls lists files in a directory.\nBy default, the command will list the filenames of the files in your current working directory. At the moment, this is probably your home directory.\nIf you add a space followed by a -l (that is, a hyphen and a small letter L), after the ls command, it alters the behavior of the command: it will now list the files in your current directory, but with details about them including who owns them, what the size is, and what kind of file it is. An example is shown in the code block below.\n\ndrwxr-xr-x 6    manager users   4096    2008-08-21  09:26  twilliams\n-rw-r--r-- 1    manager users   9784    2007-03-19  14:09  hybInfo.txt\n-rw-r--r-- 1    manager users   9784    2007-03-19  14:09  targets_v1.txt\n-rw-r--r-- 1    manager users   7793    2007-03-19  14:14  targets_v2.txt\n^      ^        ^       ^    ^         ^                ^\nFile  File    User    Group Size   Date/Time         Filename\ntype  Permission\n\nThe file permissions are given in triplets rwx which represent write - read - execute permissions. The first triplet represents the permission associated with the owner (usually the person who created the file), the second triplet is permissions for owner’s group, and the triplet is permission for everyone else.\n\nExercise: looking at file permissions\nOpen your terminal and use ls -l command to interrogate the files in your directories."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#tab-completion",
    "href": "1.1_Introduction to Linux.html#tab-completion",
    "title": "Introduction to Linux",
    "section": "Tab completion",
    "text": "Tab completion\nHave you been remembering to tab complete?\nTab completion is an incredibly useful facility for working on the command line.\nOne thing tab completion does is complete the filename or program name you want, saving huge amounts of typing time. It also ensures that there are no errors in what you typed, which is easy to do with long filenames or paths\nFor example, you could type:\n\n#navigate to classdata folder\ncd classdata\n# now start to type REFS\ncd R \n\nnow hit the tab key.\nIf there is only one directory with a name starting with the letter “R”, the rest of the name will be completed for you. Here this would give you:\n\ncd REFS\n\nUser accounts are setup such that if there is more than one file with that combination of letters, all the files will be shown to you. You can choose the one you want by typing more of the filename, or by double tapping the tab key.\n\nExercises: practise tab completion\nReturn to your home directory if you are not already there by typing cd ~/\nType cd cl and use tab completion for the rest of the command. Then press the return key.\nYou will now be in your ~/classdata directory.\nType l Ses and hit tab twice to view the files available.\nNow press the tab key again. You can gradually add extra letters and use the tab key to limit the options available.\nAs you get faster with this, it will save you a lot of typing effort.\n\n\n\n\n\n\n\nAuto completion for programs\n\n\n\nAuto completion works when you are calling programs that are available (where you have installed or loaded them with module load) and when you are constructing commands. However, auto completion does NOT work if you are in a text editor constructing a script."
  },
  {
    "objectID": "1.1_Introduction to Linux.html#putting-it-together-creating-a-mental-map-of-your-file-system",
    "href": "1.1_Introduction to Linux.html#putting-it-together-creating-a-mental-map-of-your-file-system",
    "title": "Introduction to Linux",
    "section": "Putting it together: creating a mental map of your file system",
    "text": "Putting it together: creating a mental map of your file system\nOne of the commonest reasons why students get stuck in bioinformatics exercises is that they have forgotten where there are in the file system. They try to work with files that are in a different directory, panic that things seem to have disappeared, or wonder where exactly that output has ended up! The key to saving yourself much pain lies in developing awareness of how your file structure is laid out, and where you are in it at all times. If something doesn’t work, stop and ask yourself: where am I? Where are the files I want to work with?\nYou can also make life easier for yourself by thinking carefully about giving your directories and files self-explanatory names, and keeping them organised.\nAs a final exercise in this part of the session, use the commands you have learned so far (particularly pwd, ls and cd) to sketch a map of your mydata folder."
  },
  {
    "objectID": "1.2_Loading_Software_Accessing_Data.html",
    "href": "1.2_Loading_Software_Accessing_Data.html",
    "title": "Loading Software and Accessing Data",
    "section": "",
    "text": "Now that you’re (hopefully) re-caffeinated and have stretched your legs and cleared your brain, let’s have a look at some more need-to-know information."
  },
  {
    "objectID": "1.2_Loading_Software_Accessing_Data.html#text-files-word-processors-and-bioinformatics",
    "href": "1.2_Loading_Software_Accessing_Data.html#text-files-word-processors-and-bioinformatics",
    "title": "Loading Software and Accessing Data",
    "section": "Text files, Word Processors and Bioinformatics",
    "text": "Text files, Word Processors and Bioinformatics\nDocuments written using a word processor such as Microsoft Word or OpenOffice Write are not plain text documents and will not work in either a text editor or if used as a script. If your filename has an extension such as .doc or .odt, it is unlikely to be a plain text document. (Try opening a Word document in notepad or another text editor on Windows if you want proof of this.)\nWord processors are very useful for preparing documents, but do not use them for working with bioinformatics-related files. We recommend that you prepare text files for bioinformatics analyses using Linux-based text editors and not Windows- or Mac-based text editors. This is because Windows- or Mac-based text editors may insert hidden characters that are not handled properly by Linux-based programs. There are endless posts on bioinformatics forums bemoaning the problems that arise from this!\nThere are a number of different text editors available on Bio-Linux. These range in ease of use, and each has its pros and cons. In this practical we will briefly look at two editors, nano and vi. Vi is an extremely powerful text editor and popular with many professionals, as it is installed on almost every Linux system. However, it is also notoriously frustrating to get to grips with. For this reason, you are probably better starting off with nano, which is very easy to use and displays instructions at the bottom of the screen.\n\nNano\nPros:\nVery easy – For example, command options are visible at the bottom of the window, it can be used when logged in without graphical support, and is fast to start up and use\nCons:\nBy default it puts return characters into lines too long for the screen (i.e. using nano for system administration can be dangerous!) This behavior can be changed by setting different defaults for the program or running it with the –w option. It is not completely intuitive for people who are used to graphical word processors.\n\n\nVi (or Vim)\nPros:\nAppears on nearly every Unix system. Can be very powerful if you take the time to know the key-short cuts.\nCons:\nYou have to know the shortcuts!! There’s no menus and no on screen prompts\n\n\n\n\n\n\nExercise\n\n\n\nNote that unlike most graphical programmes, where you write a document and then name it at the end, command-line text editors require you to name a file when you create it.\n\nCreate a file with nano\n\nnano test_nano.txt \n\nType some text, exit ctrl X, save and return to command line now list the contents of the file you created\n\nless test_nano.txt \n\n\n\nCreate a file with vi\n\nvi test_vi.txt \n\nVi has two separate modes: ‘input’ mode, where you can type and edit text, and ‘control’ mode where you can do things such as save, search and exit the programme.\nType ‘i’ to enter input mode, and add some text.\nPress [esc] to enter control mode.\nExit, saving your edits, by typing :wq - this stands for write and quit! If you wanted to exit without saving the edits, typing :q! will force vi to exit without saving.\nNow list the contents of the file you created\n\nless test_vi.txt"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html",
    "href": "1.3_Commandline_Tools_and_Scripting.html",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "",
    "text": "grep stands for global regular expression print; you use this command to search for text patterns in a file (or any stream of text). eg.\n\ngrep \"ency\" ~/mydata/Session1/subsample.txt\n\nYou can also use flexible search terms, known as regular expressions, in your grep searches. You have already used glob pattern expressions in this practical, but regular expressions are somewhat different and more powerful. For example, when you listed all files with the pattern testxt you were using a glob pattern comprising explicit characters (e.g. tes) and special symbols (* meaning any character or characters). The equivalent in grep would be “tes.txt.” where the period signifies any single character and the * signifies any number of repeats.\nTherefore to get from a shell glob pattern to a regular expression replace each * with .* and each ? with . . You also need to enclose the expression in quotes to tell the shell not to try and interpret it as a glob.\nUnmodified glob patterns will be accepted by grep but will not work as intended. For example the pattern tes* in grep means te followed by any number of s characters in sequence (te, tes, tess, tesss, …). The question mark now signifies optionality – so tes? means te followed by zero or one s character (te, tes). Regular expressions are found in several places other than grep, most notably in the Perl scripting language. The full syntax is extensive and powerful but is beyond the scope of this course, so back to the command itself…\ngrep requires a regular expression as input, and returns all the lines containing that pattern to you as output.\ngrep is especially useful in combination with pipes as you can filter the results of other commands.\nFor example, perhaps you only want to see only the information in an txt file relating to the origin of the sequence, that is, the DE line. You do not need to search the file in an editor, you can just grep for lines beginning in DE, as in the next exercise.\n\n\n\n\n\n\nExercise\n\n\n\nMove into mydata/Session1\nType the command:\n\ngrep \"DE\" O97435.txt \n\nWhat is this command doing?\nCan you see why the above command results in the output you see? An explanation of this command can be found below this exercise box.\nTry the commands:\n\ngrep \"^DE\" O97435.txt\ngrep -x \"DE.*\" O97435.txt \n\nWhat are the ^ symbol and the -x parameter in these commands doing? Check the man page for grep to be sure.\nTry the command:\n\ncat O97435.txt | grep \"^DE\". \n\nDoes that do what you expected?\nUse the above command with a pipe and a grep command to search for files created or modified today.\nThe first command in the above exercise searches all the text in the O97435.txt file and returns the lines in which it finds the letter D followed by the letter E.\nThe second command in the exercise also returns lines in the file that have a letter D followed by a letter E, but only where DE is found at the beginning of a line. This is because the ^ symbol means “match at the beginning of a line”. The $ symbol can be used similarly to mean “at the end of a line”. These are known as anchors. Passing the -x flag to grep tells it to automatically anchor both ends of the search pattern.\nWhat this anchoring does in the example above is return to you just the organism information in the txt file. This is because none of the other lines returned in the previous command started with DE, they just contained DE somewhere in them. This is an example where knowing how information is stored in an given file, along with a few basic Linux commands, allows you to retrieve information quickly.\nAnother common example is counting how many sequences are in a set of multi-fasta files. We can do this with pipes between the commands cat, grep and the handy wc (word count) utility, which here we use to count lines found by grep.\n\ncat subsample_Ill1.fasta | grep \"^&gt;\" | wc -l \n\ngrep -c \"^&gt;\" subsample_Ill1.fasta \n\nEach sequence in a fasta file starts with a header line that begins with a &gt; . The above command streams the contents of all files matching the glob pattern *seqs.fasta through a search with grep looking for lines that start with the symbol &gt; . The quotes around the pattern ^&gt; are necessary, as otherwise it is interpreted as a request for redirection of output to a file, rather than as a character to look for. As before, the ^ symbol means “match only at the beginning of the line”.\nThe output of this grep search is sent to the wc command, with the -l indicating that you want to know the number of lines – ie. the number of headers and by implication the number of sequences.\nAn easier version of this shown as the second example uses the grep -c argument that return the number of matches found.\nNow try this:\n\ncat *.fasta | grep \"^&gt;\" | wc -l \n\nRemember that * means any pattern, and that cat will concatentate all the files given as input. Therefore, a synopsis of the command above is: Read through all files with names ending seqs.fasta and look for all the header lines in the combined output, then count up those lines that matched and return the number to screen.\n\n\n\n\nYou can use all of grep functionality with a compressed file by adding a ‘z’ - just replace grep with zgrep and you can directly query that .gz files."
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "href": "1.3_Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "",
    "text": "You can use all of grep functionality with a compressed file by adding a ‘z’ - just replace grep with zgrep and you can directly query that .gz files."
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "href": "1.3_Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Step 1: Create your Script",
    "text": "Step 1: Create your Script\nMake a text file containing the script in question. This can be achieved by downloading or transferring the scripts as a file in the correct format. Sometimes the scripts are posted as part of a website such as a web-post in a discussion forum. To use these scripts, create a file using vi or nano and copy into the test file the script in question ensuring you save it with an appropriate name.\nHere’s a script you can try\n\n#!/bin/bash \n\necho {10..1} \n\necho 'Blast off'"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "href": "1.3_Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Step 2: Make your script executable",
    "text": "Step 2: Make your script executable\nMake the file executable. Before you can run the script you must make it executable. This is done by changing its property using\n\nchmod a+x [script name] \n\nthis is shorthand for chmod (change modify) a(all)+(add)execute(e) [script name] – thus changing the permission to allow everyone to execute a script. For more guide to chmod see https://en.wikipedia.org/wiki/Chmod"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "href": "1.3_Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Step 3: Run your script",
    "text": "Step 3: Run your script\nRun the program. This should be easy but there are a few ways of doing this.\nPlace the program into the directory where you want to use it and type\n\n./[script name] parameters arguments \n\nOn first use try to run with no parameters or arguments or with -h and -help to see the manual for the script. Some poorly written scripts will need you to define the program you need to use them, i.e. for a bash script :\n\nbash [script name] parameters arguments \n\nRun from your current location using the script’s full path.\n\n/full path/[script name] parameters arguments \n\nPlace the script into your ‘PATH’ – this means that the computer automatically knows about the script and will run it from any location just given the program name. I suggest that if you want to do this ask the demonstrators and they can show you……this is advanced as if you put two scripts with the same name into the PATH you can cause issues."
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "href": "1.3_Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Loops using numerical variables",
    "text": "Loops using numerical variables\nCreating a Loop\nInvoke a text editor such as nano, then type\n\n#!/bin/bash\n\nfor i in {1..[number]}; do \n\n# use hash to include some level of documentation so when you get to script in a few months time \n\n# you can remember what it was all about.  ${i} = number which increment by 1 each time the loop runs\n\n[your commands]${i} \n\ndone\n\nsave.\nnow make the program executable\n\nchmod +x [program_name]\n\nrun\n\n./[program_name]"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "href": "1.3_Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Loops using Strings (lists) as variables",
    "text": "Loops using Strings (lists) as variables\nInvoke a text editor such as nano, then type\n\n#!/bin/bash\n\nfor i in sampleA sampleB sampleC sampleD; do \n\n# use hash to include some level of documentation so when you get to script in a few months time\n\n# you can remember what it was all about.  ${i} = the list of strings given at the start of the for loop \n\n[your commands]${i} \n\ndone \n\nsave\nnow make the program executable\n\nchmod +x [program_name] \n\nrun\n\n./[program_name]"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "href": "1.3_Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "Loops using directory listings",
    "text": "Loops using directory listings\nThis is a great method if you want to execute a series of command on a set of data files contained in a specific directory, for instance a series of sequence files.\nInvoke a text editor such as nano, then type\n\n#!/bin/bash\n\nsequence_dir=[location of folder containing paired end sequence files]\n#*_R1.fastq - lists all files ending in _R1.fastq\n\nfor f in ${sequence_dir}/*_R1.fastq\ndo\n\n#the file name are placed in variable $f - we can separate the name of the file away from the suffix (.fastq) using this simple cut expression - the variable 'R1' now contains the file name with no suffix \nR1=$(basename $f | cut -f1 -d.)\n\n#Sometimes we want the 'base' name of the file without the direction suffix (_R1) - this expression creates a variable 'base' where the _R1 has been replace is nothing - ie removed \nbase=$(echo $R1 | sed 's/_R1//')\n\necho ${base}\n\ndone \n\nsave\nnow make the program executable\n\nchmod +x [program_name] \n\nrun\n\n./[program_name]"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#gz-files",
    "href": "1.3_Commandline_Tools_and_Scripting.html#gz-files",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "gz files",
    "text": "gz files\n….wait a minute do you really need to decompress this file !! Many programs will happily use a .gz file directly, this a win for your file space so check out if you really need to decompress the file. Unfortunate some utilits like ‘sed’ require files to be unzipped, it that case:\n\n#to decompress\ngunzip [filename].gz\n#to recompress\ngzip [filename]"
  },
  {
    "objectID": "1.3_Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "href": "1.3_Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "title": "Extension: Commandline Tools and Scripting",
    "section": "How to extract a .tar.gz file on Linux",
    "text": "How to extract a .tar.gz file on Linux\nTo extract a .tar.gz file on Linux, you can use the “tar” command in the terminal. Here is the general syntax:\n\ntar -xvzf filename.tar.gz\n\nHere is a brief explanation of the options used:\n\n-x: This option tells tar to extract the contents of the archive.\n\n-v: This option is for verbose output, which means that tar will display a list of the files being extracted as it does so.\n\n-z: This option tells tar to decompress the archive using gzip.\n\n-f: This option is used to specify the archive file to extract."
  },
  {
    "objectID": "1.x_Linux_fundamentals.html",
    "href": "1.x_Linux_fundamentals.html",
    "title": "Linux the fundamentals",
    "section": "",
    "text": "Linux/Unix commands usually take the form shown below\n\nCommand       parameters      agguments\n   ^              ^               ^\nwhat I         how I want     on what do\nwant to do     to do it       I want to do it\n\nNB usually each element is separated by only one space\nThe first item you supply on the command line is interpreted by the system as a command; that is – something the system should do. Items that appear after that on the same line are separated by spaces. The additional input on the command line indicates to the system how the command should work. For example, what file you want the command to work on, or the format for the information that should be returned to you.\nMost commands have options available that will alter the way they function. You make use of these options by providing the command with parameters, some of which will take arguments. Examples in the following sections should make it clear how this works. With some commands you don’t need to issue any parameters or arguments. Occasionally this is because there are none available, but usually this is because the command will use default settings if nothing is specified.\nIf a command runs successfully, it will usually not report anything back to you, unless reporting to you was the purpose of the command. If the command does not execute properly, you will often see an error message returned. Whether or not the error is meaningful to you depends on your experience with Linux/Unix and how user-friendly the errors generated were designed to be.\nNote: Items supplied on the command line separated by spaces are interpreted as individual pieces of information for the system. For this reason, a filename with a space in it will be interpreted as two filenames by default."
  },
  {
    "objectID": "1.x_Linux_fundamentals.html#changing-directories",
    "href": "1.x_Linux_fundamentals.html#changing-directories",
    "title": "Linux the fundamentals",
    "section": "Changing Directories",
    "text": "Changing Directories\nThe command used to change directories is cd\nIf you think of your directory structure, (i.e. this set of nested file folders you are in), as a tree structure, then the simplest directory change you can do is move into a directory directly above or below the one you are in.\n\n\n\nLinux general file structure\n\n\nIf you are using a Cardiff personal cloud qubernetes container you will have access to the following additional file structure.\n\n\n\nCardiff University Personal Cloud File Structure\n\n\n\n\n# Change to a directory to your home directory use\n\ncd ~/\n\n# Now move into teh bioinformmatics Session1 of classdata \n\ncd ~/classdata/Bioinformatics/Session1\n\n# Go down a directory\n\ncd ..\n\n# you can review where you are at any time using the print working directory command\n\npwd"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#fasta-and-fastq",
    "href": "2.1_NGS_Quality_Control.html#fasta-and-fastq",
    "title": "NGS Quality Control",
    "section": "FASTA and FASTQ",
    "text": "FASTA and FASTQ\nThe two commonest file types for sequencing data are FASTA and FASTQ. (To make life a little more complicated, both have more than one file extension. FASTAs can be .fasta, .fa or .fna; FASTQs can be .fastq or .fq)\nA FASTA is a 2-line file format. Each sequence has a header line, which always starts with &gt; and contains an identifier for the sequence. The next line contains the sequence itself. For example, a FASTA file with two sequences in it could look like this:\n\n&gt;Sequence 1 \nACGTGCTTCCGGTTTCAGGGTCA\n&gt;Sequence 2\nGTACTTAACCTAAACTGGACTAA\n\nA FASTQ is an extension of a FASTA that includes quality scores for each base of the sequence. It is a 4-line file format. The first line is still a header, but now begins with @, and the second line is the sequence. The third line is always a single + that separates the sequence from the quality scores on the fourth line. A FASTQ of those same two sequences could look like this:\n\n@Sequence 1 \nACGTGCTTCCGGTTTCAGGGTCA\n+\nR%!JSQA(AD\\@ASDIA&ASD&!N\n@Sequence 2\nGTACTTAACCTAAACTGGACTAA\n+\n&DJSA)ADFLA9DF5*J34AQBS\n\n\n\n\n\n\n\nExercise: looking at sequence files\n\n\n\n\nCopy the Session2 folder from classdata to mydata, and cd into it.\nRead the file subsample_Ill1.fasta using one (or more!) of the commands cat, less, head and tail.\nUsing your method of choice, look at Illumina_1.fastq. Can you see the difference?\n\nDon’t forget to use tab completion!"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#pipes",
    "href": "2.1_NGS_Quality_Control.html#pipes",
    "title": "NGS Quality Control",
    "section": "Pipes",
    "text": "Pipes\nVery often in Linux you want to use the output of one command as the input to the next. This can easily be done using the pipe (|) character. For example, suppose I want to count the number of sequences in a FASTA file. I can do this easily using a combination of the search command grep and the word count command wc.\n\ncat myFile.fasta | grep \"&gt;\" | wc -l\n\nLet’s break down what this command is doing. First of all, I access the whole content of myFile.fasta using cat. However, rather than dumping it onto the screen I redirect that to become the input to grep. I tell grep to search for the symbol &gt; and sift out only the lines which contain that symbol. Because this is a FASTA, I know that can only be the header lines. I then feed this output into the wc command, with the option -l to count lines rather than words.\n\n\n\n\n\n\nExercise: using the pipe\n\n\n\n\nTry using the command above to count the number of sequences in subsample_Ill1.fasta.\nWhat happens if you remove | wc -l from the end? Do you understand why?\n\n\n\nPipes are particularly useful as they allow you to run…. LOOPS!"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#loops",
    "href": "2.1_NGS_Quality_Control.html#loops",
    "title": "NGS Quality Control",
    "section": "Loops",
    "text": "Loops\nLoops are one of the best things about working on a Linux system. They allow you to write a command once and then run it on any number of files. There are several kinds of loops which all work on the same principles, so we will focus on the while loop. Let’s look at a simple loop and break it down.\n\nls *.fasta | while read file; do wc -l ${file}; done\n\nThis starts by listing all the FASTA files in the directory Session2 (remember that * stands in for anything). Try running\n\nls *.fasta \n\njust to check you’re happy with that.\nThe next step is to use a pipe. Instead of just listing the FASTA files on the screen, we are going to use that as input for our loop. The loop starts while read file. This means that one at a time, every line of that input will take a turn at being substituted for the word ‘file’ in the command that will follow. You don’t have to call it ‘file’ - it could be banana or profiterole or socks; the only thing that matters is that you use the same word throughout the loop. Whatever word you choose, it becomes something known as a variable: literally, a value that can vary as it stands in for something else. while read sets the value of file, and then whenever we want to access its contents we can do so using ${file}.\nHaving set the loop in progress, a semicolon indicates that next comes the command we want to run for each value of our variable. In this case, I am using wc -l to count the total number of lines in the file. Another semicolon followed by ‘done’ is needed to finish the loop and tell it to go away and run.\n\n\n\n\n\n\nExercise: using loops\n\n\n\n\nTry running the loop above in the Session2 directory.\nTry changing ‘file’ to something else. Does it work?\n[Extension] Using what you’ve learned so far, can you make a loop that will only count the sequence header lines?"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#fastqc",
    "href": "2.1_NGS_Quality_Control.html#fastqc",
    "title": "NGS Quality Control",
    "section": "FastQC",
    "text": "FastQC\nIn the pantheon of great bioinformatics software, FastQC has a special place. It’s free, simple to use and has been around for ages, and everyone uses it because it’s just so good. FastQC will take your FASTQ file and produce a nice easy-to-read report about it with loads of information.\nTo use FastQC on the server, we will first need to load the module. This is the equivalent of starting up a programme on your desktop computer. You can find out what modules are available on the server by typing the command\n\nmodule avail\n\nYou should see that the list includes fastqc/0.11.9. To load this module, simply type\n\nmodule load fastqc/0.11.9\n\nTry this out now!\nAs mentioned before, FastQC is very easy to run. At its most basic (without any extra options), you simply type (substituting file.fastq for the name of the FASTQ file you want to check):\n\nfastqc file.fastq\n\nThere is one extra option that is useful to include, and that is the -o flag. This stands for ‘output’ and tells FastQC what directory to put its output files in.\n\nfastqc -o output_dir file.fastq\n\n\n\n\n\n\n\nExercise: using FastQC\n\n\n\n\nMake a directory called fastqc inside your Session2 folder.\nTry running FastQC on Illumina_1.fastq, using your fastqc directory for the output.\nUse ls to see what output it has created.\nUsing the MobaXterm sftp file transfer window or FileZilla, transfer the html file to your desktop and open it.\n\nReminder: if using FileZilla, the details are:\n\nHost: sponsa.bios.cf.ac.uk\nUsername: your username\nPassword: your three word password\nPort: Your assigned port number\n\n\n[Extension] Look up the help page using fastqc -h. What other options are available?\n\n\n\n\n\n\n\n\n\nWorked Example: Quality Control example for a single fastq file"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#running-and-reusing-scripts",
    "href": "2.1_NGS_Quality_Control.html#running-and-reusing-scripts",
    "title": "NGS Quality Control",
    "section": "Running and Reusing Scripts",
    "text": "Running and Reusing Scripts\nSo far you have just been typing commands directly into the command line. This is great for exploring directories etc., but now you are starting to do some proper analysis you’ll soon find yourself asking questions like\n\nwhat if I want to run this again?\nwhat if I want to check exactly what I did?\nwhat if I want to show someone else what I did?\n\nThese are excellent questions, and they can all be answered by writing SCRIPTS! If that sounds a bit daunting, a script is simply a text file that you write your commands into. By adding a special line of code called a shebang, the computer can run the commands in this text file as though you had typed them directly onto the command line.\nThe shebang is simply\n\n#!/usr/bin/bash\n\n\n\n\n\n\n\nExercise: a simple script\n\n\n\nLet’s create a file with nano and write a simple script in it. By convention, script files are given the extension .sh rather than .txt\n\nnano test_nano.sh\n\n\ntype the shebang at the top #!/usr/bin/bash\nType ls on the next line\nExit using ctrl-X, save and return to command line\nNow we need to make the file executable (so it can be run as a programme)\n\n\nchmod a+x [script name]\n\nthis is shorthand for chmod (change modify) a(all)+(add)execute(x) [script name] – thus changing the permission to allow everyone to execute a script. For a very helpful guide to chmod see https://en.wikipedia.org/wiki/Chmod\nNow run the program!\n\n./test_nano.sh\n\nWhat output do you get? It is what you expected?"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#quality-trimming-using-fastp",
    "href": "2.1_NGS_Quality_Control.html#quality-trimming-using-fastp",
    "title": "NGS Quality Control",
    "section": "Quality trimming using fastp",
    "text": "Quality trimming using fastp\nHaving assessed the quality of our sequence data, the next thing we want to do is clean it up. There are many programmes available for the task, but we will use the one-stop-shop software fastp which has won many friends by being both comprehensive and easy to use. It has an excellent users’ guide at https://github.com/OpenGene/fastp.\nThe main things fastp does is to trim off bad bases and remove leftover adaptor sequences. It automatically identifies the adapters so you don’t need to tell it in advance what to look for.\n\n\n\n\n\n\nExercise: fastp\n\n\n\nCreate a new file using nano, copy the script below, make it executable and run it! We are going to use the forward and reverse reads for our sample to run fastp in paired-end mode.\nScript:\n\n#!/usr/bin/bash\n\nmodule load fastp/0.20.0\n\nfastp -q 20 -u 10 --cut_right -i Illumina_1.fastq -I Illumina_2.fastq -o Illumina_1trimmed.fastq -O Illumina_2trimmed.fastq\n\nLook up the help page for fastp. What does each of the options given above mean?\nUse ls to look at the output. How could the script be improved to make it tidier?\n\n\n\n\n\n\n\n\nExercise: checking it worked!\n\n\n\n\nRun FastQC on your trimmed Illumina_1 file and compare it to the original. Can you see the difference?\nIn the Session2 directory there is a sub-directory called looping, with five single-end Illumina samples in it. Can you write a script which will use loop over those files to do QC on them, then trim them and re-run the QC? Extra brownie points for keeping the output directories nice and tidy!"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#creating-a-quality-summary-with-multiqc",
    "href": "2.1_NGS_Quality_Control.html#creating-a-quality-summary-with-multiqc",
    "title": "NGS Quality Control",
    "section": "Creating a quality summary with multiqc",
    "text": "Creating a quality summary with multiqc\nMultiqc will search through a parent and all daughter folders, extracting all the qc data and creating a graphical summary. Try it for your session folder.\n\n module load multiqc/1.16\n \n #run multiqc on all files in the current working directory\n multiqc .\n \n\nuse the -h flag to see what option multiqc provides.\n\n\n\n\n\n\nWorked Example: Fastqc interactive loop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorked Example: Fastqc bash loop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorked Example: Multiqc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: the main event!\n\n\n\nYou are now going to meet the dataset we’ll be exploring for the next few days. In ~/classdata there is a folder called datasets. Copy this folder across to your ~/mydata directory\n\ncp -r ~/classdata/datasets ~/mydata/datasets\n\nInside this folder you will find two data sets: prokaryote and mitochondrial. We will be using the prokaryote dataset to clean, assemble, annotate and analyse genomes over the next few days. The mitochondrial dataset will be for you to play with in the extension sessions.\nYour challenge now is to write script(s) to run fastQC, fastp, fastQC (again!) and multiqc on the prokaryote files. You can either copy them over to mydata/Session2 or work directly from mydata/datasets/prokaryotes. There are multiple prokaryote files, so you will need to use loops!\nIf you are feeling confident, try creating one script that will carry out all the processes on all the files. Remember to keep your file names and output directories tidy!\n\n\n\n\n\n\n\n\nWorked Example - Prokayote: rawdata fastqc loop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorked Example - Prokayote: rawdata QC triming and post trim QC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtension Work\n\n\n\nDo the same for the mitochondrial data!"
  },
  {
    "objectID": "2.1_NGS_Quality_Control.html#time-for-coffee.-you-have-earned-yourselves-cake",
    "href": "2.1_NGS_Quality_Control.html#time-for-coffee.-you-have-earned-yourselves-cake",
    "title": "NGS Quality Control",
    "section": "Time for coffee. You have earned yourselves cake!",
    "text": "Time for coffee. You have earned yourselves cake!"
  },
  {
    "objectID": "2.2_genome_assembly.html",
    "href": "2.2_genome_assembly.html",
    "title": "Genome Assembly",
    "section": "",
    "text": "We will now assemble the genomes for the mystery prokaryotes in the datasets folder. You can either copy them over to mydata/Session2 or work directly from mydata/datasets/prokaryotes (I suggest sticking with whichever approach you used for QC and trimming). Whichever you choose to do, always remember to keep your folder system tidy. If you are struggling to remember how it’s arranged, grab some paper and sketch it out as a visual reminder.\nEither way, write scripts rather than running the programmes by typing directly into the terminal. This means you have every step saved so it can be revisited whenever you need."
  },
  {
    "objectID": "2.2_genome_assembly.html#genome-assembly",
    "href": "2.2_genome_assembly.html#genome-assembly",
    "title": "Genome Assembly",
    "section": "Genome assembly",
    "text": "Genome assembly\nWe will assemble the processed reads into an assembly using the assembler Unicycler. Unicycler is an assembly pipeline for bacterial (and mitochondrial) genomes. It can assemble Illumina-only read sets where it functions as a SPAdes-optimiser.\n\nmodule load unicycler/v0.5.0\n\nBasic usage:\n\nunicycler -1 [processed fastq R1] -2 [processed fastq R2]  -t [thread number] -o [output directory]\n\nEach assembly will be located in a different directory within the parent directory indicated by the -o option\n\n\n\n\n\n\nExercise: Unicycler\n\n\n\nWrite a script to assemble your genome(s) with Unicycler.\nIf working on more than one genome: rename assembly files and copy to new directory\nA big part of bioinformatics is maintaining directory and file organisation. Each mitochondrial genome assembly output by Unicycler will have the same generic name, assembly.fasta, albeit located in a different folder. We need to rename these files to reflect the input data. Compose a loop to copy and rename these files to a new directory."
  },
  {
    "objectID": "2.2_genome_assembly.html#assembly-statistics",
    "href": "2.2_genome_assembly.html#assembly-statistics",
    "title": "Genome Assembly",
    "section": "Assembly statistics",
    "text": "Assembly statistics\nOnce our assembly files have been renamed and copied to a single location, we can analyse them for quality statistics. Use Quast to report useful metrics such as assembly length, GC content, contig number, and N50 value. QUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics.\n\nmodule load quast/5.2.0\n\nBasic usage:\n\nquast.py -t [thread number] -o [output directory] [input fasta file]\n\nThe output directory contains the report in multiple formats. To view the report on the terminal use the cat command which prints the contents of a file to the terminal (don’t forget to navigate to the file first as it’s in a different directory!):\n\ncat [report.txt]\n\n\n\n\n\n\n\nExercise: Quast\n\n\n\nWrite a script to assess your genome assembly with Quast.\n\n\n\n\n\n\n\n\nOptional: assembly graph\n\n\n\n\n\nOne of the files produced by Unicycler is an assembly graph (.gfa file extension). This file details the links between contigs that were produced during the assembly and can provide valuable information on the difficult-to-assemble regions of the genome. Bandage is a program for visualising de novo assembly graphs. By displaying connections which are not present in the contigs file, Bandage opens up new possibilities for analysing de novo assemblies. Download to your local laptop from the link: https://rrwick.github.io/Bandage/."
  },
  {
    "objectID": "2.3_hybrid_assemblies.html",
    "href": "2.3_hybrid_assemblies.html",
    "title": "Extension: Hybrid Assemblies",
    "section": "",
    "text": "So far, you have been assembling genomes just using Illumina short reads. However, as discussed in the lecture, sometimes there are areas of genome that can’t be resolved with short reads, meaning the assembly produces more contigs. This can be improved by adding in long reads to scaffold the contigs. The short reads provide high quality detail, while the long reads provide the big-picture context.\nUnicycler provides the option to add in long-read information very simply by using the -l flag to direct it to a file of long reads.\n\n\n\n\n\n\nExcercise: SPades Hybrid\n\n\n\nWe have provided you with Nanopore long-read data for your prokaryote genomes in ~/classdata/datasets/prokayote/prokaryote_NP/.\n\nRun a hybrid assembly with Nanopore data, then use Quast to compare the results to short-read-only assembly.\n\nHybrid assembly example command line:\n\nunicycler -1 [processed fastq R1] -2 [processed fastq R2] -l [nanopore fastq] -t [thread number] -o [output directory] 1&gt; [file_name]\n\nNotice that at the end of the command I’ve added 1&gt; [file_name]. This tells bash that instead of just spitting messages onto the screen as the command runs, it should write them to a file. Unicycler produces some useful and informative output while running hybrid assembly, which it is helpful to keep.\n\nHave a read through Unicycler’s information page on hybrid assembly. How does it use the long reads to bring gaps in the short-read assembly?\nUse less to look at the log file you’ve produced. Can you use the Unicycler page above to interpret what it’s telling you?"
  },
  {
    "objectID": "3.1_Genome_visualisation_and_annotation.html#use-bedtools-to-extract-out-your-phylogenetic-marker-sequence-e.g.-16s-or-coi-from-your-assembled-and-annotated-genome.",
    "href": "3.1_Genome_visualisation_and_annotation.html#use-bedtools-to-extract-out-your-phylogenetic-marker-sequence-e.g.-16s-or-coi-from-your-assembled-and-annotated-genome.",
    "title": "Genome Visualisation and Annotation",
    "section": "Use bedtools to extract out your phylogenetic marker sequence (e.g. 16S or CoI) from your assembled and annotated genome.",
    "text": "Use bedtools to extract out your phylogenetic marker sequence (e.g. 16S or CoI) from your assembled and annotated genome.\nCollectively, the bedtools utilities are a Swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line.\n\nmodule load bedtools2/2.31.0\n\nWe can use bedtools to extract gene/protein sequences from annotation files:\n\nNavigate to either the Prokka annotation output directory or the Barrnap output file\nCreate a sub-gff file that contains the annotation information of only the 16 rRNA gene\n\n\n# For bacterial sequences\ngrep \"product=16S ribosomal RNA\" [gff annotation file]  &gt; [subset_16S.gff]\n\n# For mitochondrial sequences\ngrep \"ID=gene_cox1\" [gff annotation file]  &gt; [subset_coi.gff]\n\n\nExtract the 16S rRNA gene from the mitochondrial genome assembly using the subset_16S.gff annotation file (contains the co-ordinates for the start/stop positions). Depending on how the assembly graph was made, the sequence may be in the opposite orientation (reverse complement). To account for this we need to extract based on the strandedness using the -s option.\n\n\nbedtools getfasta -fi [input fasta file] -bed [subset_gff file] -s &gt; [output 16S file.fasta]\n\nThe fasta sequence will be given a numeric nonsensical name (what comes after the &gt;). Change this for the name of the source sequence.\n\nsed -i \"s/&gt;.*/&gt;[sequence name]/g\" [output 16S file.fasta]\n\n\n\n\n\n\n\nExercise: Extracting 16S genes\n\n\n\nWrite a loop that uses the bedtools steps to extract the 16S sequence from all your prokaryote genomes."
  },
  {
    "objectID": "3.1_Genome_visualisation_and_annotation.html#blast",
    "href": "3.1_Genome_visualisation_and_annotation.html#blast",
    "title": "Genome Visualisation and Annotation",
    "section": "BLAST",
    "text": "BLAST\nBLAST is old-school sequence searching! You will probably have BLASTed things before via their web interface. However, for something as big as a genome you will need to run BLAST locally (i.e. on the server rather than via the web). This requires having a database downloaded to search against. The teaching servers have a choice of databases downloaded in ~/classdata/REFS/blastdb/.\n• swissprot – Uniprot annotated protein database\n• mito_pro – metazoan mitochondrial database\n• 16S_ribosomal_RNA – 16S ribosomal database\n• hum_mt_pep – human mitochondrial\nDon’t forget to load BLAST and any other modules you need!\n\nmodule load blast-plus/2.13.0"
  },
  {
    "objectID": "3.1_Genome_visualisation_and_annotation.html#blast-programs",
    "href": "3.1_Genome_visualisation_and_annotation.html#blast-programs",
    "title": "Genome Visualisation and Annotation",
    "section": "BLAST Programs",
    "text": "BLAST Programs\nBLAST comes in a variety of flavours, depending on what you want to search and what you want to search it against. The first thing you need to decide when BLASTing is which one you want.\n\n\n\n\n\n\n\n\nProgram\nInput-Output\nDescription\n\n\n\n\nblastn\nnucleotide-nucleotide\nThis program, given a DNA query, returns the most similar DNA sequences from the DNA database that the user specifies.\n\n\nblastp\nprotein-protein\nThis program, given a protein query, returns the most similar protein sequences from the protein database that the user specifies.\n\n\nblastx\nnucleotide\n6-frame translation-protein This program compares the six-frame conceptual translation products of a nucleotide query sequence (both strands) against a protein sequence database.\n\n\ntblastx\nnucleotide\n6-frame translation-nucleotide 6-frame translation This program is the slowest of the BLAST family. It translates the query nucleotide sequence in all six possible frames and compares it against the six-frame translations of a nucleotide sequence database. The purpose of tblastx is to find very distant relationships between nucleotide sequences.\n\n\ntblastn\nprotein-nucleotide\n6-frame translation This program compares a protein query against the all six reading frames of a nucleotide sequence database.\n\n\npsi-blast\nposition-specific\nThis program is used to find distant relatives of a protein. First, a list of all closely related proteins is created. These proteins are combined into a general “profile” sequence, which summarises significant features present in these sequences. A query against the protein database is then run using this profile, and a larger group of proteins is found. This larger group is used to construct another profile, and the process is repeated. By including related proteins in the search, PSI-BLAST is much more sensitive in picking up distant evolutionary relationships than a standard protein-protein BLAST.\n\n\nmegablast\nlarge queries seqs\nWhen comparing large numbers of input sequences via the command-line BLAST, “megablast” is much faster than running BLAST multiple times. It concatenates many input sequences together to form a large sequence before searching the BLAST database, then post-analyze the search results to glean individual alignments and statistical values"
  },
  {
    "objectID": "3.1_Genome_visualisation_and_annotation.html#anatomy-of-the-blast-command",
    "href": "3.1_Genome_visualisation_and_annotation.html#anatomy-of-the-blast-command",
    "title": "Genome Visualisation and Annotation",
    "section": "Anatomy of the BLAST command",
    "text": "Anatomy of the BLAST command\nA basic BLAST command states your reference database, your query, and an output file, for example:\n\nblastx –db /home/db/fish –query contigs.fasta –out contigs_blx.txt\n\nThis example uses blastx to match a DNA query against protein database, targets a (fictional) protein database named ‘fish’ stored at /home/db/, blasts the file contig.fasta and puts the output in contig_blx.txt\nGiven that you will be searching a nucleotide query against a nucleotide database, which BLAST command do you think you need?\n\n\n\n\n\n\nExtension:Making your own BLAST databases\n\n\n\n\n\nYou already have the databases you require today, but sometimes you will need to create a custom database based on some collection of sequences dear to your heart! It’s not as simple as just creating a FASTA of sequences to search against: in order for BLAST to use them, it has to convert them into BLAST database format. The command to do so is makeblastdb. To create a new database:\n\n    makeblastdb -in nucleotide_seq.fa –dbtype nucl –title my_db -out my_seq\n\n# Command line options\n-in         [input_file] \n-dbtype     [molecule_type], nucl – nucleotide or prot - protein\n-title  [database_title]\n-out        [database_name]\n\nThere are also many other options to explore if you so desire!\n\n\n\n\n\n\n\n\n\nExercise: Searching with BLAST\n\n\n\n\nPerform BLAST analysis of one of your extracted 16S genes against the 16S_ribosomal_RNA database. Interrogate the output using ‘less’.\nRe-run the BLAST analysis with the output format parameter set to a customised -outfmt 6, which will output the data into a tablular format (often useful for subsequent analyses). Call the output something different so as not to overwrite the previous one. Interrogate the output using ‘less’. Can you see the difference to previously? How easy do you find it to read and interpret?\n\n\n    blastx -query [contigs.fasta] -db ~/classdata/REFS/blastdb/hum_mt_pep -out [output.txt] -outfmt \"6 qseqid sseqid pident length mismatch gapopen sstart send qstart qend evalue bitscore\"\n\n\nHave a look at this guide to further customising your output table. Can you get it to return a table that just contains the query ID, the subject (match) ID, the species name and the E value? N.B. To get BLAST to return taxonomic information, you need to copy the taxonomy database into your working directory.\n\n\ncp ~/classdata/REFS/blastdb/taxdb.* .\n\n\nLook at the BLAST options using blastn -help. Can you find one that will allow you to choose how many hits it returns?\nExtension: Produce a loop to get identities for all your prokaryotes.\n\n\n\n\n\n\n\n\n\nExtension: mapping reads with minimap2\n\n\n\n\n\n[Minimap](https://github.com/lh3/minimap2) is a mapping software. It maps (aligns) sequencing reads to an assembled genome, so that it’s possible to see where in the genome each read originated from. We can use to see how the individual reads contributed to the assembled genome, and how deep the coverage is at each point along the genome.\n\nmodule load minimap2/2.14\n\nSamtools - BAM/SAM tool utilities (for handling the alignment files minimap produces). BAM and SAM are file formats for storing reads alongside lots of information about how each read maps to the genome. They are very similar, but BAM files are more compact and not human-readable (if you try to open one if looks like gobbledygook), whereas SAM files are larger but human-readable (at least to a certain type of nerd).\n\nmodule load samtools/1.16.1\n\nAlign read sequences to your assemblies and create a BAM file.\n\nminimap2 -ax sr [your contigs.fasta] [forward_read.fastq] [reverse_read.fastq] &gt; aln.sam\nsamtools view -b -S aln.sam &gt; aln.bam\nsamtools sort aln.bam &gt; aln_sorted.bam\nsamtools index aln_sorted.bam\n\nOverlay the BAM file you have generated into your assembly using Artemis.\n\n\n\n\n\n\n\n\n\nWorked Example - Prokayote annotation loop"
  },
  {
    "objectID": "3.2_Annotating_mitochondria.html",
    "href": "3.2_Annotating_mitochondria.html",
    "title": "Extension: Annotating Mitochondrial Genomes",
    "section": "",
    "text": "Now is a good opportunity to look back over anything you’re unclear on and would like to go over again. If you haven’t already annotated all your prokaryote genomes with Prokka, now is the time to do so. If you’ve done all that and are chomping at the bit for more, read on for a tutorial on how to annotate your mitochondrial assemblies."
  },
  {
    "objectID": "3.2_Annotating_mitochondria.html#extract-fasta-sequence-for-genes-in-bed-file",
    "href": "3.2_Annotating_mitochondria.html#extract-fasta-sequence-for-genes-in-bed-file",
    "title": "Extension: Annotating Mitochondrial Genomes",
    "section": "Extract Fasta sequence for genes in Bed file",
    "text": "Extract Fasta sequence for genes in Bed file\nYou can extract the fasta sequence for the genes in the bed file using bedtools getfasta:\n\nmodule load bedtools2/2.31.0\n\nbedtools getfasta –fi contigs.fasta -bed result.bed -name &gt; result.fasta\n\nOr using a loop\n\nfor i in {1..3}; \ndo \nbedtools getfasta -fi spades_output_${i}/before_rr.fasta -bed mitos_${i}/result.bed -name &gt;mitos_${i}_result.fasta\ndone\n\n\n\n\n\n\n\nExercises: Annotate your mitochondria with mitos2\n\n\n\nFollow these steps to annotate your mitochondrial genome using mitos:\n\nUse seqtk seq to select only the largest contig into a new file\nUse mitos to annotate the contig, and visualize it with Artemis\nCreate a loop to generate the output for all mitochondrial assemblies\n[Extension] Use the BEDtools to extract the sequence of the annotated genes\n[Extension] Use seqtk to extract just the COX1 gene\n\n\n\n\n\n\n\n\n\nExtension: Annotate your mitochondria with BLAST\n\n\n\nTake your assembled mitochondrial genome and identify mitochondrial orthologues using blast.\n\nPerform blastx analysis of your sequence against the mitochondrial protein database. Interrogate the using ‘less’.\nRe-run the blastx analysis with the output format parameter set to a cutomised -outfmt 6, which will output the data into a tablular format compatible with Artemis.\n\n\n    blastx -query [contigs.fasta] -db ~/classdata/REFS/blastdb/hum_mt_pep -out [output.txt] -outfmt \"6 qseqid sseqid pident length mismatch gapopen sstart send qstart qend evalue bitscore\"\n\nYou can now open the fasta sequence you have used in the blast with Artemis and then overlay with the clast output file.\n\nRepeat the blastx analysis of your sequence against the mitochondrial protein database [mito_pro] only returning blast matches with an E value lower that 1E-10 and using all 4 threads.\nRepeat the blastx analysis of your sequence against the mitochondrial protein database, only returning blast matches with an E value lower that 1E-10, using all 4 threads, limiting the outputs so that you keep to 100 and putting the outputs into the tabular format compatible with Artemis.\n[Extension] Produce a loop to annotate all assembled contigs using blast\n[Extension] You can try repeating this exercise with swisprot and note the differences"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html",
    "href": "4.1_phylogentics_phylogenomics.html",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "",
    "text": "Online Phylogenetic Lecture"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#use-bedtools-to-extract-out-your-phylogenetic-marker-sequence-e.g.-16s-or-coi-from-your-assembled-and-annotated-genome.",
    "href": "4.1_phylogentics_phylogenomics.html#use-bedtools-to-extract-out-your-phylogenetic-marker-sequence-e.g.-16s-or-coi-from-your-assembled-and-annotated-genome.",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Use bedtools to extract out your phylogenetic marker sequence (e.g. 16S or CoI) from your assembled and annotated genome.",
    "text": "Use bedtools to extract out your phylogenetic marker sequence (e.g. 16S or CoI) from your assembled and annotated genome.\nYesterday you used bedtools to extract the 16S rRNA gene sequences from your prokaryote genomes (and maybe also to extract the CO1 genes from your mitochondria).\n\n\n\n\n\n\nExercise: Extracting 16S genes\n\n\n\n\nMove your extracted 16S rRNA gene sequence into the directory of 16S rRNA reference genes provided.\nCombine all the 16S rRNA gene fasta files into a single fasta file\n\n\ncat [*.fasta] &gt; [output multifasta file]"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#use-mafft-to-create-an-alignment-of-your-phylogenetic-marker.",
    "href": "4.1_phylogentics_phylogenomics.html#use-mafft-to-create-an-alignment-of-your-phylogenetic-marker.",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Use MAFFT to create an alignment of your phylogenetic marker.",
    "text": "Use MAFFT to create an alignment of your phylogenetic marker.\nMAFFT is a multiple sequence alignment program for Unix-like operating systems.\n\nmodule load mafft/7.505\n\nBasic usage:\n\nmafft [input multifasta] &gt; [output alignment multifasta]\n\n\n\n\n\n\n\nExercise: Aligning sequences\n\n\n\nUse MAFFT to align the 16S sequences you have extracted."
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#construct-a-phylogeny-with-raxml-ng",
    "href": "4.1_phylogentics_phylogenomics.html#construct-a-phylogeny-with-raxml-ng",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Construct a phylogeny with RAxML-NG",
    "text": "Construct a phylogeny with RAxML-NG\nRAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML) optimality criterion.\n\nmodule load RAxML-NG/v1.2.0\n\nRAxML-NG outputs files to your current location. Create a directory and run RAxML from the directory to keep files organised\nBasic usage for nucleotide alignment with GTR model:\n\nraxml-ng --all --msa [input alignment] --model GTR --bs-tree [bootstrap replicate number e.g. 100] --prefix [prefix for output files] --threads [number of threads/cpus]\n\nRAxML-NG produces multiple output files. The file we are interested in has the extension .support. Use mv to change the extension .nwk on this file to indicate it is a phylogeny.\n\n\n\n\n\n\nExercise: Creating a tree\n\n\n\nUse RAxML-NG to create a tree from your aligned sequences. Start with a bootstrap value of 100.\nRe-run the tree with 500 and then 1000 bootstraps and compare the results. What do they tell you about the phylogeny?"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#visualize-your-tree-with-figtree",
    "href": "4.1_phylogentics_phylogenomics.html#visualize-your-tree-with-figtree",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Visualize your tree with Figtree",
    "text": "Visualize your tree with Figtree\nFigTree is designed as a graphical viewer of phylogenetic trees and as a program for producing publication-ready figures.\nDownload locally to your laptop: https://github.com/rambaut/figtree/releases / or access by logging into the server using -X\n\nmodule load figtree/1.4.4\n\nfigtree\n\nAn interactive window will now open for you to use if you are using X11 enabled mobaxterm. If you are using a mac or linux shell you will need to install figtree (Download locally to your laptop: https://github.com/rambaut/figtree/releases) locally and download the newick file (nwk suffix).\nWhen you open your newick if you will be asked about node labeling. You should identify that your node labeling is you bootstrap values:\nLabeling Fig Tree Branches \n\n\n\n\n\n\nWorked Example: creating a 16S prokayote phylogeny"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#single-gene",
    "href": "4.1_phylogentics_phylogenomics.html#single-gene",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Single Gene",
    "text": "Single Gene"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#whole-genome-snp",
    "href": "4.1_phylogentics_phylogenomics.html#whole-genome-snp",
    "title": "Phylogenetics and Phylogeneomics",
    "section": "Whole Genome SNP",
    "text": "Whole Genome SNP"
  },
  {
    "objectID": "5.1_RNAseq_processing.html#alignment",
    "href": "5.1_RNAseq_processing.html#alignment",
    "title": "RNAseq Processing",
    "section": "Alignment",
    "text": "Alignment\nSTAR – (Spliced Transcript Alignments to a Reference) is an alignment package which functions similarly to standard genome alignments, but is designed for short regions of RNA that could span intron-exon junctions and with low compute requirements. STAR outputs a bam format file which contains the locations where all the reads in your dataset have aligned and which genes they cover."
  },
  {
    "objectID": "5.1_RNAseq_processing.html#counting",
    "href": "5.1_RNAseq_processing.html#counting",
    "title": "RNAseq Processing",
    "section": "Counting",
    "text": "Counting\nFeatureCounts is a simple package that takes the positions of mapped reads and outputs a file quantifying the expression of each gene or exon (based on parameter choices). At this point raw read counts are hard to interpret due to likely different levels of sequencing achieved per sample and methodological biases.\nOne common step prior to counting is marking duplicates that arise from data generation for further information, or so that they can be removed. Here we’ll use the imaginatively named MarkDuplicates from GATK."
  },
  {
    "objectID": "5.1_RNAseq_processing.html#differential-gene-analysis",
    "href": "5.1_RNAseq_processing.html#differential-gene-analysis",
    "title": "RNAseq Processing",
    "section": "Differential Gene Analysis",
    "text": "Differential Gene Analysis\nContrasting the expression profile of the samples is typically done with one of two R packages: Deseq2 or EdgeR (the mac vs windows of the RNAseq fight), however a multitude of alternatives exist. These packages perform the normalization and statistical steps of contrasting samples as defined in a metadata file stating your experimental design (replicates, tissue type, treatment etc). The output here is a range of significant genes, ordinance and cluster analysis of sample similarity, and various quality control figures.\nFollowing these three steps, there are an almost infinite number of tools and packages to look deeper into your data, find experimentally specific insights, and prior published data to contrast against."
  },
  {
    "objectID": "5.1_RNAseq_processing.html#software-and-scripts",
    "href": "5.1_RNAseq_processing.html#software-and-scripts",
    "title": "RNAseq Processing",
    "section": "Software and scripts",
    "text": "Software and scripts\nWe will be using scripts to run these steps. In the classdata/Session5/scripts folder you will find the following that you can use as a basis for your analysis, however make sure you’re tuning it to your own file structure and file names.\nWe’ll be using a full sized RNAseq sample otherwise it causes the programs to think it’s bad data. In the classdata/Day3 folder there four pairs of RNAseq files from an Arabidopsis RNAseq study. In the folder classdata/REFS there is a reference genome, and a gtf file. The step 2 “star index genome” has already been run for you (you don’t need to do this!)\nScripts\n\n1-QC.sh\n2-star_index_genome.sh (already done, don’t repeat!)\n3-star.sh\n\n4-markduplicates.sh\n\n5-featurecounts.sh\n\n\n\n\n\n\n\nExercises: RNAseq processing\n\n\n\nUsing the pre-made scripts provided, perform the steps on four pairs of fastq files. There are examples of all of these files in the classdata/Session5 directory which you should copy into your own folder. You will need to edit them to represent your own working folder and filenames:\n\nQC and trim your sample data\nUse the outputs from your QC script as inputs to run star.\nUse the outputs from star to run mark duplicates to both remove and keep duplicates (this will generate two outputs).\nUse the outputs to each of these to run featureCounts on the data.\nRun multiQC on the processed directory and observe the summary.\n\nThese outputs are now ready to put into R and perform Differential Gene Expression Analysis. Things to remember:\n\nLoad any modules you will need!\nUse the full path any time it could be ambiguous."
  },
  {
    "objectID": "5.1_RNAseq_processing.html#star_index_genome.sh",
    "href": "5.1_RNAseq_processing.html#star_index_genome.sh",
    "title": "RNAseq Processing",
    "section": "2-star_index_genome.sh",
    "text": "2-star_index_genome.sh\n\n#!/bin/bash\n\n# Load some modules\nmodule load star/2.7.6a\n \n## Useful shortcuts\nexport refdir=~/classdata/REFS\n\n## Change --sjdbOverhang to length of your sequence data /2 minus 1\n\n\necho \"\\n\\n I TOLD YOU NOT TO RUN THIS ONE NOW! \\n\\n (unless you're in the future and trying to run this for real, in which case you need to edit this script and remove the # characters from the command)\"\n\nSTAR    --runThreadN 8 \\\n        --limitGenomeGenerateRAM 321563573 \\\n        --runMode genomeGenerate \\\n        --genomeDir  $refdir/ \\\n        --genomeFastaFiles $refdir/Arabidopsis_thaliana.TAIR10.dna_sm.toplevel.fa \\\n        --sjdbGTFfile $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        --sjdbOverhang 49"
  },
  {
    "objectID": "5.1_RNAseq_processing.html#star.sh",
    "href": "5.1_RNAseq_processing.html#star.sh",
    "title": "RNAseq Processing",
    "section": "3-star.sh",
    "text": "3-star.sh\n\n#!/bin/bash\n\n## Load some Modules\nmodule load star/2.7.6a\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\nexport refdir=~/classdata/Bioinformatics/REFS/STAR/\n\n## The commands you want to run\nmkdir $workingdir/star\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\nfor i in ${list[@]}\ndo\n# map forward and reverse reads to genome\n# If input data is gzipped (.fastq.gz) inculde the additional parameter:   --readFilesCommand zcat\nSTAR   --outMultimapperOrder Random \\\n       --outSAMmultNmax 1 \\\n       --runThreadN 4  \\\n       --runMode alignReads \\\n       --outSAMtype BAM Unsorted \\\n       --quantMode GeneCounts \\\n       --outFileNamePrefix $workingdir/star/${i}-unsort. \\\n       --genomeDir $refdir \\\n       --readFilesIn $workingdir/fastq/${i}-trim_1P.fastq $workingdir/fastq/${i}-trim_2P.fastq\ndone"
  },
  {
    "objectID": "5.1_RNAseq_processing.html#markduplicates.sh",
    "href": "5.1_RNAseq_processing.html#markduplicates.sh",
    "title": "RNAseq Processing",
    "section": "4-markduplicates.sh",
    "text": "4-markduplicates.sh\n\n#!/bin/bash\n\n#load some modules\nmodule load picard/2.26.2\nmodule load samtools/1.15.1\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\n\nmkdir markdup\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\nfor i in ${list[@]}\ndo\nsamtools index $workingdir/star/${i}-unsort.Aligned.out.bam\nsamtools sort -@ 4 -o $workingdir/star/${i}.sorted.bam $workingdir/star/${i}-unsort.Aligned.out.bam\n\n##  MARK DUPLICATES  ##\npicard MarkDuplicates I=$workingdir/star/${i}.sorted.bam O=$workingdir/markdup/${i}.markdup.bam M=$workingdir/markdup/${i}.metrics.markdup.txt REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT\n\n\n## REMOVE DUPLICATES ##\npicard MarkDuplicates I=$workingdir/star/${i}.sorted.bam O=$workingdir/markdup/${i}.rmdup.bam M=$workingdir/markdup/${i}.metrics.rmdup.txt REMOVE_DUPLICATES=true VALIDATION_STRINGENCY=SILENT\n\ndone"
  },
  {
    "objectID": "5.1_RNAseq_processing.html#featurecounts.sh",
    "href": "5.1_RNAseq_processing.html#featurecounts.sh",
    "title": "RNAseq Processing",
    "section": "5-featurecounts.sh",
    "text": "5-featurecounts.sh\n\n#!/bin/bash\n\n# Load some modules\nmodule load subread/2.0.2\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\nexport refdir=~/classdata/Bioinformatics/REFS/STAR/\n\n\nmkdir $workingdir/featureCounts\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\n\n\nfor i in ${list[@]}\ndo\n\nfeatureCounts \\\n        -T 4 -p -F GTF -t exon -g gene_id \\\n        -a $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        -o $workingdir/featureCounts/${i}.markdup.featurecount \\\n        $workingdir/markdup/${i}.markdup.bam\n\nfeatureCounts \\\n        -T 4 -p -F GTF -t exon -g gene_id \\\n        -a $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        -o $workingdir/featureCounts/${i}.rmdup.featurecount \\\n        $workingdir/markdup/${i}.rmdup.bam\n\ndone"
  },
  {
    "objectID": "5.2_DEG_analysis.html#converting-between-common-gene-ids",
    "href": "5.2_DEG_analysis.html#converting-between-common-gene-ids",
    "title": "Differential Gene Expression Analysis",
    "section": "Converting between common gene IDs",
    "text": "Converting between common gene IDs\n\nbiodbnet\ngprofiler convert"
  },
  {
    "objectID": "5.2_DEG_analysis.html#whole-dataset-annotation-and-ontologies",
    "href": "5.2_DEG_analysis.html#whole-dataset-annotation-and-ontologies",
    "title": "Differential Gene Expression Analysis",
    "section": "Whole dataset annotation (and ontologies)",
    "text": "Whole dataset annotation (and ontologies)\n\ngProfiler\nKEGG\nNeVOmics"
  },
  {
    "objectID": "5.2_DEG_analysis.html#interaction-networks",
    "href": "5.2_DEG_analysis.html#interaction-networks",
    "title": "Differential Gene Expression Analysis",
    "section": "Interaction Networks",
    "text": "Interaction Networks\n\nStringDB\nGOnet\nCytoscape\n\nbingo plugin\nEnrichmentMap plugin"
  },
  {
    "objectID": "5.2_DEG_analysis.html#other-visualisation-tools",
    "href": "5.2_DEG_analysis.html#other-visualisation-tools",
    "title": "Differential Gene Expression Analysis",
    "section": "Other visualisation tools",
    "text": "Other visualisation tools\n\nMorpheus\nComplex venn"
  },
  {
    "objectID": "5.2_DEG_analysis.html#whole-packages",
    "href": "5.2_DEG_analysis.html#whole-packages",
    "title": "Differential Gene Expression Analysis",
    "section": "Whole packages",
    "text": "Whole packages\n\nIPA\nShiny-Seq\nBeavR"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html",
    "href": "5.3_DEG_Functional_Interpretation.html",
    "title": "DEG Functional Interpretation",
    "section": "",
    "text": "Workshop Recording to come"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#processing-rnaseq-data",
    "href": "5.3_DEG_Functional_Interpretation.html#processing-rnaseq-data",
    "title": "DEG Functional Interpretation",
    "section": "Processing RNAseq Data",
    "text": "Processing RNAseq Data\nConverting RNASeq data into gene counts is the first-step prior to analysis of the biological function and networks revealed through the subsequent transcription analysis. This process is outside the scope of this workshop but if you are interested Andres et al 2013 Provides an excellent overview of the processes involved.\n Figure 1. Process overview"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#dataset-to-use---geogds2565",
    "href": "5.3_DEG_Functional_Interpretation.html#dataset-to-use---geogds2565",
    "title": "DEG Functional Interpretation",
    "section": "Dataset to use - Geo:GDS2565",
    "text": "Dataset to use - Geo:GDS2565"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#panopto_walk_through",
    "href": "5.3_DEG_Functional_Interpretation.html#panopto_walk_through",
    "title": "DEG Functional Interpretation",
    "section": "Panopto_walk_through",
    "text": "Panopto_walk_through"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#false-discover-rate",
    "href": "5.3_DEG_Functional_Interpretation.html#false-discover-rate",
    "title": "DEG Functional Interpretation",
    "section": "False Discover Rate",
    "text": "False Discover Rate"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#fishers-exact-test-and-enrichment-analysis",
    "href": "5.3_DEG_Functional_Interpretation.html#fishers-exact-test-and-enrichment-analysis",
    "title": "DEG Functional Interpretation",
    "section": "Fisher’s Exact Test and Enrichment Analysis",
    "text": "Fisher’s Exact Test and Enrichment Analysis"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#geo-workshop",
    "href": "5.3_DEG_Functional_Interpretation.html#geo-workshop",
    "title": "DEG Functional Interpretation",
    "section": "Geo Workshop",
    "text": "Geo Workshop\n\nSearch for you dataset\n\nGo to Geo DataSets: (https://www.ncbi.nlm.nih.gov/gds/)[https://www.ncbi.nlm.nih.gov/gds/]\n GEO_DataSet Search\nnow select \n\nRefine search to only show ‘DataSets’ and ‘Series’\n\nSelect &lt; DataSets and Series &gt; from the left hand menu.\n Refine to DataSets\n3A. Select procesed dataset (should be number 3 in list and have a heat map icon to the right)\nClick on title &lt; Endothelial cell response to ultrafine particles &gt; to select DataSet\n Entry Page\nTake notes off all pertinent information about the experiment, including species and what microarray platform was used. In this case experiment was conducted on rats and analysed on an Affymetrix Human Genome U133 Plus 2.0 Array. Download any publication available. Also, if you are interested you could have a look at the cluster analysis on the right hand site. This will show you how the relationship of the expression profiles from each sample relative to each other. If the experiment was successful, all samples of a certain treatment should cluster together.\n4A. Compare experiment samples\nClick &lt;Compare 2 sets of sample&gt;\nChoose &lt;test e.g. One-tailed t-test (A &gt; B)&gt;\nChoose _ e.g. 0.001\nClick on: Step 2: Select which Samples to put in Group A and Group B\n Select Groups to compare\nChoose &lt; Query Group A vs B &gt;\nYou should now see a list of the following DEGS\n DEG List\n5A. Download DEGs\nYou have &gt;100 DEGs but the page only displays the first 20. Before downloading DEGS change the items per page from 20 to 500.\n Items Per Page\nSelect &lt; Download profile data &gt;\nand you will be prompted to save the profiling of the DEGs displayed as default file name &lt;profile_data.txt&gt;\nSave to appropriate location. If needed Select Page 2…. of the DEGS and repeat the download process.\nRepeat this process for &lt;test e.g. One-tailed t-test (B &gt; A) significance level 0.001 &gt;_\n6A. Open and Merge DEG lists in Excel\nThe DEG lists show the gene list with there relative expression level (normalised) and annotation for the genes involved (annotation shown in columns BG -&gt;). For Our next steps we will use the Gene symbol that is in Column BH.\n3B. Select un-processed series (should be number 2 - it will have icon &lt; Analyze with GEO2R &gt; at the end of the entry)\nClick on &lt; Analyze with GEO2R &gt;\nNow Define groups by click clicking the &lt; Define Group pull &gt; down and create groups ‘control’ and ‘Treatment’ (enter group name and press enter). Click on each sample in list and associated it with one of your group (you can hold ctrl down to select multiple entry before associating them with the group).\n Select Groups to compare\n4B. Customise the Option and Analyse\nSelect &lt; Options &gt; and customise as shown below:\n Geo2R Options\nSelect &lt; reanalyze &gt;\nWill will now see a Processing icon - this may take a minute or two.\n5B. DEGs\nYou will now see a table and a series of Visualisations - review the visualisation taking note of what each are showing you.\n Geo2R results\nVenn diagram showing GSE4567: Limma, Padj&lt;0.05 - 704 genes - this is the set we will use\nClick on &lt; Explore and download, control vs treatment and Download Significant genes &gt;\nThis will give you a Tsv you can open in excel\n6B DEG TSV\nOpen the DEG TSV - you will have ID (Affymetrix), Gene Symbol, Description and Log2(fold Change) and Adjusted p-value. The Gene.symbol can have multiple symonyms for each gene - this can bias future analsyis. Copy/paste gene symbol column to rught hand column (so no other data is on right), and use &lt; Data &gt; Text to Columns &gt; Delimited &gt; Other &gt; ‘/’ &gt; Finish &gt;_ to push secondary symbols into other columns."
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#geo-database-panopto",
    "href": "5.3_DEG_Functional_Interpretation.html#geo-database-panopto",
    "title": "DEG Functional Interpretation",
    "section": "GEO Database Panopto",
    "text": "GEO Database Panopto"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#gprofiler-panopto",
    "href": "5.3_DEG_Functional_Interpretation.html#gprofiler-panopto",
    "title": "DEG Functional Interpretation",
    "section": "gprofiler panopto",
    "text": "gprofiler panopto"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#gene-enrichment-analysis-with-david",
    "href": "5.3_DEG_Functional_Interpretation.html#gene-enrichment-analysis-with-david",
    "title": "DEG Functional Interpretation",
    "section": "Gene Enrichment Analysis with David",
    "text": "Gene Enrichment Analysis with David\n[David - Database for Annotation, Visualization and Integrated Discovery] (https://david.ncifcrf.gov/) was the tool to use for gene enrichment from 2005 - 2016 but the database it hosted became out of date due to a break in the research groups funding. From 2016 (gprofiler)[https://biit.cs.ut.ee/gprofiler/gost] and (stringdb)[https://string-db.org/] have been prefered because they have been kept upto date. However, DAVID was refunded and a 2021 update means it is back to being maintained. Although there will not be time to explore it in our workshop the following workshop and video is there to support anyone wanting to explore its functionality."
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#start-analysis-and-upload-you-data",
    "href": "5.3_DEG_Functional_Interpretation.html#start-analysis-and-upload-you-data",
    "title": "DEG Functional Interpretation",
    "section": "1. Start analysis and upload you data",
    "text": "1. Start analysis and upload you data\nSelect &lt; start analysis &gt;\nPaste in you gene list (1) in box (A) and, select identifier (2), and identify as a Gene List (3). Select your species (2a) . In more nuanced analysis you may wish to define your own background - this is useful when working with non-model organisms or if your starting population does not represent the entire genome.\n DAVID Upload\nnow select &lt; Submit List &gt;"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#generate-functional-analysis-of-gene-list",
    "href": "5.3_DEG_Functional_Interpretation.html#generate-functional-analysis-of-gene-list",
    "title": "DEG Functional Interpretation",
    "section": "2. Generate Functional Analysis of gene list",
    "text": "2. Generate Functional Analysis of gene list\nIf you have used a non-regulate gene identifier or it does not recognize the identifier type you have used it may ask you to convert the identifiers - check the programs suggestion - it is usually good but you should check.\nSelect &lt; Functional Annotation Tool &gt;\nYou will now be give a annotation summary as shown below:\n Annotation Summary\nYou will see a range of categories for which the functional annotation has been performed. The analysis told has not only cross referenced the gene list against a range of functional databases it has also analysed the gene co-appearance in citations (Literature), links to disease (Disease) and Interactions agonist other linkages."
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#review-gene-ontology-enrichment",
    "href": "5.3_DEG_Functional_Interpretation.html#review-gene-ontology-enrichment",
    "title": "DEG Functional Interpretation",
    "section": "3. Review Gene Ontology Enrichment",
    "text": "3. Review Gene Ontology Enrichment\nLet us consider the functional enrichment using (Gene Ontology)[http://geneontology.org/]. Gene Ontology categorizes gene products using three distinct characteristics - Molecular Function (biochemistry of the product), Cellular Component (where it appears in the cell), and the Biological Processes - see more about these classification by reviewing the (Gene Ontology Overview documentation)[http://geneontology.org/docs/ontology-documentation/]. Although ontologies are hierarchical, they are not a simple classification system, they not only allow for gene products to be involved with multiple processes the relationship between elements in the hierarchy are closely defined. The ontology also allows for classification to be as specific or detailed as knowledge allows ie if a protein is a transporter but what it transports is not known it will be defined by a mid-level term ‘transporter’ but if it is known to transport Zn it will be classified at a ‘Zinc transport’ as well as a ‘transporter’. Enrichment analysis uses fisher exact test (see about) to calculate the enrichment at each of these ‘levels’ - although I usually choose to look at the integrated data summarized under the ‘GOTERM_[BP/CC/MF]_DIRECT’.\nclick on the + next too the &lt; Gene_Ontology &gt; category\nSelect &lt; Chart &gt; right of the GOTERM_BP_DIRECT\n BP Enrichment\nNote the P-value and benjamini correct p-value displaying the like hood that those specific Go terms are represented by random - i.e. lost the P-value the less likely the representation of the term is a random select , therefore the more likely the term is enriched.\nTo see the list of gene involved in for instance ‘positive regulation of osteoblast differentiation’ click on the blue bar under the ‘Genes’ column.\nThis is the list you will see:\n positive regulation of osteoblast differentiation\nNow review enrichment for Cell Component and Molecular Function."
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#pathway-analysis",
    "href": "5.3_DEG_Functional_Interpretation.html#pathway-analysis",
    "title": "DEG Functional Interpretation",
    "section": "4. Pathway analysis",
    "text": "4. Pathway analysis\nclick on the + next too the &lt; Pathways &gt; category\n Pathway Options\nSelect &lt; Chart &gt; right of the KEGG\n KEGG Enrichment\nYou will see a list of enriched pathways - Note the P-value and benjamini correct p-value displaying the like hood that those specific pathways are represented by random - i.e. lost the P-value the less likely the representation of the pathway is a random select, therefore the more likely the pathways is enriched.\nclick on the &lt; TNF Signalling Pathway &gt;_ in the term column\nThis will display the pathway with the terms that are present in the list shown with red stars or highlight by redtext below the figure.\n TNF Signalling Pathway\nExplore some more pathways"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#functional-annotation-clustering",
    "href": "5.3_DEG_Functional_Interpretation.html#functional-annotation-clustering",
    "title": "DEG Functional Interpretation",
    "section": "5. Functional Annotation Clustering",
    "text": "5. Functional Annotation Clustering\nDAVID attempts to summarise enrichment between categorization systems using a tool it terms - Functional annotation clustering. If you select this for you current data set. you will be provided with the following clusters:\n Functional Annotation Clustering\nEach cluster is assigned an enrichment score under which ‘terms’ that are enriched under different classification systems are displayed grouped together. Each ‘grouping’ is given a ‘Enrichment Score’ the larger the enrichment score the higher the score the more likely that cluster is being enriched. Note that the is a ‘Classification Stringency’ pull down menu that allows you to define the strength of associated of the term being grouped together.\nFor annotation Cluster 6 (which contains lots of Metallothionein/cadimum associated terms) there is a small green and black box - after the Enrichment Score and the ‘G’ - click on this box. This brings up a cluster matrix showing the gene gene products on the Y-axis and the classification ‘vlasses’ on the X-axis.\n Cluster Matrix\nDavid Workshop\n\nEnrichment with David Panopto"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#string-panopto",
    "href": "5.3_DEG_Functional_Interpretation.html#string-panopto",
    "title": "DEG Functional Interpretation",
    "section": "STRING Panopto",
    "text": "STRING Panopto"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#export-go-terms-and-p-values",
    "href": "5.3_DEG_Functional_Interpretation.html#export-go-terms-and-p-values",
    "title": "DEG Functional Interpretation",
    "section": "1. Export GO terms and P-values",
    "text": "1. Export GO terms and P-values\n\n1A. gprofiler\nUnder the detailed results menu there is a CSV icon which when clicked can be used to export your enriched terms as a CSV which can then be imported into excel. You can use the associated setting button (the cog symbol next toi the CSV) to select only GO terms to export for this exercise.\n gprofiler export\nYou will then need to copy column C and D into Revigo\n\n\n1B. DAVID\nAfter you have completed the functional analysis select &lt; Gene_ontology &gt; GOTERM_BP_DIRECT &gt; Chart &gt;\nnow Select &lt; Download File &gt;\n DAVID Go Download\nThis will either display the enrichment table into the browser window or ask for a save location depending on your browser settings. If you are not asked for a save location right hand click the displayed table and select Save as and save to a appropriate location as a text file. This text file can be opened/imported into excel. You will file the GO term is concatenated with the GO description in Column B. To split this insert a blank column after column B, then select column B and select menu option &lt; Data &gt; Test to columns &gt;. Select &lt; Delimited &gt; Next &gt;_ and use the radio button to select other and add a ~_ to the box directly to the right of this option. Now click &lt; Next and Finish &gt;. You will see the GO term in now in Column B and the P-value in column F. Select these columns and paste them into Revigo.\nYou should now repeat this process for Molecular Function and Cell Component Terms.When all the data is merged you can paste the GO term and P value into Revigo."
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#revigo-data-entry",
    "href": "5.3_DEG_Functional_Interpretation.html#revigo-data-entry",
    "title": "DEG Functional Interpretation",
    "section": "2. Revigo data entry",
    "text": "2. Revigo data entry\nEnter GO terms and P-values into Revigo - they should be in format\nTerm    PValue\nGO:0045892  7.75E-06\nGO:0006694  3.29E-04\nLeave the default settings and select &lt; Start Revigo &gt;"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#review-revigo-results",
    "href": "5.3_DEG_Functional_Interpretation.html#review-revigo-results",
    "title": "DEG Functional Interpretation",
    "section": "3. Review Revigo Results",
    "text": "3. Review Revigo Results\nFor each GO term category Revigo will generate a Scatterplot, Table, 3D scatter plot, interactive Graph and Tree Map. Note: at the bottom of each plot there are export options for R and other formats _.\nThe most intuitive format is are the Tree Maps (see below) whilst the Scatterplot output table, which include terms like ‘Frequency and Uniqueness’ can be used to create networks in Cytoscape.\n Revigo BP TreeMap"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#revigo-workshop",
    "href": "5.3_DEG_Functional_Interpretation.html#revigo-workshop",
    "title": "DEG Functional Interpretation",
    "section": "Revigo Workshop",
    "text": "Revigo Workshop"
  },
  {
    "objectID": "5.3_DEG_Functional_Interpretation.html#revigo-panopto",
    "href": "5.3_DEG_Functional_Interpretation.html#revigo-panopto",
    "title": "DEG Functional Interpretation",
    "section": "Revigo Panopto",
    "text": "Revigo Panopto"
  },
  {
    "objectID": "6.1_metabarcoding.html",
    "href": "6.1_metabarcoding.html",
    "title": "Metagenomics",
    "section": "",
    "text": "Metabarcoding Powerpoint\nThis workshop is based heavily on QIIME’s own tutorial, Moving Pictures of the Human Microbiome. I have just modified this tutorial slightly to tailor it to our context and computer system."
  },
  {
    "objectID": "6.1_metabarcoding.html#importing-data-in-qiime",
    "href": "6.1_metabarcoding.html#importing-data-in-qiime",
    "title": "Metagenomics",
    "section": "Importing data in QIIME",
    "text": "Importing data in QIIME\nQIIME handles data by importing it into its own file format, called a QIIME artefact. This contains not only the data itself but a record of all the processes that it has gone through. Start by loading up QIIME.\n\nmodule load qiime2/2022.8 \n\nThe first step in the analysis is to read the data in.\nYou will see that the import command below has backslashes at the end of each line. This isn’t specific to QIIME, but a universal way to break up a complex command into multiple lines so that it’s easier to read. In Linux, a backslash is called an escape character; it means “interpret the next character literally”. Normally pressing enter at the end of a command tells the computer to run that command. Typing a backslash before pressing enter tells the computer that you literally just want a line break.\nLet’s explore the parts of the command\n\nqiime tools import is the QIIME programme to be used\ntype EMPSingleEndSequences tells QIIME the format the sequences will be in\ninput-path emp-single-end-sequences gives the name of the directory the sequence files are in\noutput-path emp-single-end-sequences.qza gives the name of the artefact file to be created. Note the file extension: .qza\n\n\nqiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path emp-single-end-sequences \\\n  --output-path emp-single-end-sequences.qza\n\nHenceforth, I have removed some parts of the script! Every time you see YOURFILE, it’s up to you to replace it with the correct file name."
  },
  {
    "objectID": "6.1_metabarcoding.html#demultiplexing",
    "href": "6.1_metabarcoding.html#demultiplexing",
    "title": "Metagenomics",
    "section": "Demultiplexing",
    "text": "Demultiplexing\nAs mentioned above, this step would normally be done automatically be the sequencing centre. However, it’s good to have a go at demultiplexing as there are still occasions when it needs to be done manually.\nThis command takes the QIIME artefact we have just created and uses the barcode information to decide which sample each sequence belongs to. It creates another QIIME artefact containing the demultiplexed samples.\n\nqiime demux emp-single \\\n  --i-seqs YOURFILE \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column barcode-sequence \\\n  --o-per-sample-sequences demux.qza \\\n  --o-error-correction-details demux-details.qza\n\nWe can then ask QIIME to produce some summary statistics. The output here is a .qzv: this is a QIIME visualisation file. Download a qzv and then drag and drop it into view.qiime2.org/\n\nqiime demux summarize \\\n  --i-data demux.qza \\\n  --o-visualization demux.qzv\n\nHave a look at the qzv file. What does it tell you about the data?"
  },
  {
    "objectID": "6.1_metabarcoding.html#quality-filtering-and-counting",
    "href": "6.1_metabarcoding.html#quality-filtering-and-counting",
    "title": "Metagenomics",
    "section": "Quality filtering and counting",
    "text": "Quality filtering and counting\nThe next step in the process does two things at once. Firstly, it quality checks the data. It removes any PhiX reads leftover from the sequencing, checks for low quality reads, and removes chimeras (hybrid reads created when two PCR products get erroneously stuck together). Secondly, it counts how many times each unique sequence (ASV) occurs in each sample.\nQIIME offers a few different pipelines for this step, but we are going to use one called DADA2. This is the most computationally intensive step, so be prepared for it to take up to 10 minutes to complete.\nHave a look at the help page for this command at https://docs.qiime2.org/2022.8/plugins/available/dada2/denoise-single/\n\nWhat are the p-trim-left and p-trunc-len options doing? Would there be a better way to have dealt with this problem in the data? HINT: it would need to be done before reading into QIIME!\n\n\nqiime dada2 denoise-single \\\n  --i-demultiplexed-seqs YOURFILE \\\n  --p-trim-left 0 \\\n  --p-trunc-len 120 \\\n  --o-representative-sequences rep-seqs.qza \\\n  --o-table table.qza \\\n  --o-denoising-stats stats.qza\n\nThis command produces three outputs. One is the table (how many reads per ASV per sample). Another is the representative sequences: for each ASV, it marries up the identifier with the actual sequence. Finally, there are some stats on the process.\nHaving produced our outputs, we can run some summaries to see how it’s gone.\n\nqiime feature-table summarize \\\n  --i-table YOURFILE \\\n  --o-visualization table.qzv \\\n  --m-sample-metadata-file YOURFILE\n  \nqiime feature-table tabulate-seqs \\\n  --i-data YOURFILE \\\n  --o-visualization rep-seqs.qzv\n\nLook at these qzv files. What information does each give you?"
  },
  {
    "objectID": "6.1_metabarcoding.html#taxonomy-assignment",
    "href": "6.1_metabarcoding.html#taxonomy-assignment",
    "title": "Metagenomics",
    "section": "Taxonomy assignment",
    "text": "Taxonomy assignment\nHaving counted the number of times each sequence occurs in each sample, we really want to know what organism that sequence came from. QIIME uses a machine learning tool (a Naive Bayes classifier, if you’re into that kind of thing) to assign a taxonomic identity to each sequence. The classifier is trained by giving it a database of sequences of known identity. This approach is endlessly flexible, as you can train the classifier to any type of sequence you are interested in. However, we will be using a pre-trained classifier as our data relates to a commonly-used 16S rRNA region. This classifier has been trained on the Silva database of 16S rRNA genes, focussing in just on the 515-806 region targeted by our primers.\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier silva-138-99-515-806-nb-classifier.qza\\\n  --i-reads rep-seqs.qza \\\n  --o-classification taxonomy.qza\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization taxonomy.qzv"
  },
  {
    "objectID": "6.1_metabarcoding.html#barplots",
    "href": "6.1_metabarcoding.html#barplots",
    "title": "Metagenomics",
    "section": "Barplots",
    "text": "Barplots\nNow we can get QIIME to make its famous barplots! You will see these in many, many microbiome papers.\n\nqiime taxa barplot \\\n  --i-table YOURFILE \\\n  --i-taxonomy YOURFILE \\\n  --m-metadata-file YOURFILE \\\n  --o-visualization taxa-bar-plots.qzv\n\nDownload the qzv and have a play with the options!"
  },
  {
    "objectID": "6.1_metabarcoding.html#diversity-exploration",
    "href": "6.1_metabarcoding.html#diversity-exploration",
    "title": "Metagenomics",
    "section": "Diversity exploration",
    "text": "Diversity exploration\nQIIME will also compute a lot of other statistics on your data. I personally prefer to do this in R, but for the sake of completeness let’s also look at the QIIME output (I admit, some of it is quite pretty).\n\nqiime diversity core-metrics \\\n  --i-table YOURFILE \\\n  --p-sampling-depth 1103 \\\n  --m-metadata-file YOURFILE \\\n  --output-dir core-metrics-results\n\n\nqiime emperor plot \\\n  --i-pcoa core-metrics-results/bray_curtis_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-custom-axes days-since-experiment-start \\\n  --o-visualization core-metrics-results/bray-curtis-emperor-days-since-experiment-start.qzv"
  },
  {
    "objectID": "6.1_metabarcoding.html#references",
    "href": "6.1_metabarcoding.html#references",
    "title": "Metagenomics",
    "section": "References",
    "text": "References\nBokulich NA, Kaehler BD, Rideout JR, et al. Optimizing taxonomic classification of marker‐gene amplicon sequences with QIIME 2’s q2‐feature‐classifier plug. Microbiome. 2018a;6:90\nBolyen E, Rideout JR, Dillon MR, et al. 2019. Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2. Nature Biotechnology 37: 852–857. https://doi.org/10.1038/s41587-019-0209-9\nCallahan BJ, McMurdie PJ, Rosen MJ, et al. DADA2: high‐resolution sample inference from Illumina amplicon data. Nature Methods 2016;13:581‐583.\nCaporaso JG, Lauber CL, Costello EK, Berg-Lyons D, Gonzalez A, Stombaugh J, Knights D, Gajer P, Ravel J, Fierer N, Gordon JI, Knight R. Moving pictures of the human microbiome. Genome Biology 2011;12(5):R50. doi: 10.1186/gb-2011-12-5-r50. PMID: 21624126; PMCID: PMC3271711.\nCaporaso, J. G., Lauber, C. L., Walters, W. A., Berg-Lyons, D., Lozupone, C. A., Turnbaugh, P. J., Noah Fierer, N., & Knight, R. (2011). Global patterns of 16S rRNA diversity at a depth of millions of sequences per sample. Proceedings of the Natural Academy of Sciences USA 108, 4516–4522. http://doi.org/10.1073/pnas.1000080107\nCaporaso, J. G., Lauber, C. L., Walters, W. A., Berg-Lyons, D., Huntley, J., Fierer, N., Owens, S. M., Betley, J., Fraser, L., Bauer, M., Gormley, N., Gilbert, J. A., Smith, G., & Knight, R. (2012). Ultra-high-throughput microbial community analysis on the Illumina HiSeq and MiSeq platforms. ISME Journal 6, 1621–1624. http://doi.org/10.1038/ismej.2012.8"
  },
  {
    "objectID": "6.1_metabarcoding.html#workshop-run-through",
    "href": "6.1_metabarcoding.html#workshop-run-through",
    "title": "Metagenomics",
    "section": "Workshop Run-Through",
    "text": "Workshop Run-Through"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html",
    "href": "Commandline_Tools_and_Scripting.html",
    "title": "Commandline Tools and Scripting",
    "section": "",
    "text": "{bash include=FALSE} knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "href": "Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "title": "Commandline Tools and Scripting",
    "section": "Use Zgrep with compressed files",
    "text": "Use Zgrep with compressed files\nYou can use all of grep functionality with a compressed file by adding a ‘z’ - just replace grep with zprep and you can directly query that .gz files."
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#text-files-word-processors-and-bioinformatics",
    "href": "Commandline_Tools_and_Scripting.html#text-files-word-processors-and-bioinformatics",
    "title": "Commandline Tools and Scripting",
    "section": "Text files, Word Processors and Bioinformatics",
    "text": "Text files, Word Processors and Bioinformatics\nDocuments written using a word processor such as Microsoft Word or OpenOffice Write are not plain text documents. If your filename has an extension such as .doc or .odt, it is unlikely to be a plain text document. (Try opening a Word document in notepad or another text editor on Windows if you want proof of this.)\nWord processors are very useful for preparing documents, but we recommend you do not use them for working with bioinformatics-related files.\nWe recommend that you prepare text files for bioinformatics analyses using Linux-based text editors and not Windows- or Mac-based text editors. This is because Windows- or Mac-based text editors may insert hidden characters that are not handled properly by Linux-based programs.\nThere are a number of different text editors available on Bio-Linux. These range in ease of use, and each has its pros and cons. In this practical we will briefly look at two editors, nano and vi. ## Nano\nPros:\nvery easy – For example, command options are visible at the bottom of the window can be used when logged in without graphical support fast to start up and use\nCons:\nby default it puts return characters into lines too long for the screen (i.e. using nano for system administration can be dangerous!) This behavior can be changed by setting different defaults for the program or running it with the –w option. It is not completely intuitive for people who are used to graphical word processors"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#vi-or-vim",
    "href": "Commandline_Tools_and_Scripting.html#vi-or-vim",
    "title": "Commandline Tools and Scripting",
    "section": "Vi (or Vim)",
    "text": "Vi (or Vim)\nPros:\nAppears on nearly every Unix system. Can be very powerful if you take the time to know the key-short cuts.\nCons:\nYou have to know the shortcuts!! There’s no menus and no on screen prompts\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a file with nano\n```{bash}\nnano test_nano.txt \n```\ntype some text, exit ctrl X, save and return to command line now list the contents of the file you created\n```{bash}\nless test_nano.txt \n```\n\n\nCreate a file with vi\nvi test_vi.txt \ntype ‘a’ and you can then add text\nexit saving you edits [esc]:wq! - this stands for write quit !!\nnow list the contents of the file you created\n```{bash}\nless test_vi.txt \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "href": "Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "title": "Commandline Tools and Scripting",
    "section": "Step 1: Create your Script",
    "text": "Step 1: Create your Script\nMake a text file containing the script in question. This can be achieved by downloading or transferring the scripts as a file in the correct format. Sometimes the scripts are posted as part of a website such as a web-post in a discussion forum. To use these scripts, create a file using vi or nano and copy into the test file the script in question ensuring you save it with an appropriate name.\nHere’s a script you can try\n```{bash}\n#!/bin/bash \n\necho {10..1} \n\necho 'Blast off' \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "href": "Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "title": "Commandline Tools and Scripting",
    "section": "Step 2: Make your script executable",
    "text": "Step 2: Make your script executable\nMake the file executable. Before you can run the script you must make it executable. This is done by changing its property using\n```{bash}\nchmod a+x [script name] \n```\nthis is shorthand for chmod (change modify) a(all)+(add)execute(e) [script name] – thus changing the permission to allow everyone to execute a script. For more guide to chmod see https://en.wikipedia.org/wiki/Chmod"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "href": "Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "title": "Commandline Tools and Scripting",
    "section": "Step 3: Run Your Script",
    "text": "Step 3: Run Your Script\nRun the program. This should be easy but there are a few ways of doing this.\nPlace the program into the directory where you want to use it and type\n```{bash}\n./[script name] parameters arguments \n```\nOn first use try to run with no parameters or arguments or with -h and -help to see the manual for the script. Some poorly written scripts will need you to define the program you need to use them. Ie if it was a perl program (you may have to module load perl before you run this example).\n```{bash}\nperl [script name] parameters arguments \n```\nRun from scripts current location using full path.\n```{bash}\n/full path/[script name] parameters arguments \n```\nPlace the script into your ‘PATH’ – this means that the computer automatically knows about the script and will run it from any location just given the program name. I suggest that if you want to do this ask the demonstrators and they can show you……this is advanced as if you put two scripts with the same name into the PATH you can cause issues."
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using numerical variables",
    "text": "Loops using numerical variables\nCreating a Loop\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nfor i in {1..[number]}; do \n\n# use hash to include some level of documentation so when you get to script in a few months time \n\n# you can remember what it was all about.  ${i} = number which increment by 1 each time the loop runs\n\n[your commands]${i} \n\ndone\n```\nsave.\nnow make the program executable\n```{bash}\nchmod +x [program_name]\n```\nrun\n```{bash}\n./[program_name]\n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using Strings (lists) as variables",
    "text": "Loops using Strings (lists) as variables\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nfor i in sampleA sampleB sampleC sampleD; do \n\n# use hash to include some level of documentation so when you get to script in a few months time\n\n# you can remember what it was all about.  ${i} = the list of strings given at the start of the for loop \n\n[your commands]${i} \n\ndone \n```\nsave\nnow make the program executable\n```{bash}\nchmod +x [program_name] \n```\nrun\n```{bash}\n./[program_name] \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using directory listings",
    "text": "Loops using directory listings\nThis is a great method if you want to execute a series of command on a set of data files contained in a specific directory, for instance a series of sequence files.\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nsequence_dir=[location of folder containing paired end sequence files]\n#*_R1.fastq - lists all files ending in _R1.fastq\n\nfor f in ${sequence_dir}/*_R1.fastq\ndo\n\n#the file name are placed in variable $f - we can separate the name of the file away from the suffix (.fastq) using this simple cut expression - the variable 'R1' now contains the file name with no suffix \nR1=$(basename $f | cut -f1 -d.)\n\n#Sometimes we want the 'base' name of the file without the direction suffix (_R1) - this expression creates a variable 'base' where the _R1 has been replace is nothing - ie removed \nbase=$(echo $R1 | sed 's/_R1//')\n\necho ${base}\n\ndone \n```\nsave\nnow make the program executable\n```{bash}\nchmod +x [program_name] \n```\nrun\n```{bash}\n./[program_name] \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#gz-files",
    "href": "Commandline_Tools_and_Scripting.html#gz-files",
    "title": "Commandline Tools and Scripting",
    "section": "gz files",
    "text": "gz files\n….wait a minute do you really need to decompress this file !! Many programs will happily use a .gz file directly, this a win for your file space so check out if you really need to decompress the file. Unfortunate some utilits like ‘sed’ require files to be unzipped, it that case:\n```{bash}\n#to decompress\ngunzip [filename].gz\n#to recompress\ngzip [filename]\n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "href": "Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "title": "Commandline Tools and Scripting",
    "section": "How to extract a .tar.gz file on Linux?",
    "text": "How to extract a .tar.gz file on Linux?\nTo extract a .tar.gz file on Linux, you can use the “tar” command in the terminal. Here is the general syntax:\n```{bash}\ntar -xvzf filename.tar.gz\n```\nHere is a brief explanation of the options used:\n```{bash}\n-x: This option tells tar to extract the contents of the archive.\n\n-v: This option is for verbose output, which means that tar will display a list of the files being extracted as it does so.\n\n-z: This option tells tar to decompress the archive using gzip.\n\n-f: This option is used to specify the archive file to extract.\n```"
  },
  {
    "objectID": "index.html#artifical-intelligence-ai-and-writing-code",
    "href": "index.html#artifical-intelligence-ai-and-writing-code",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Artifical intelligence (AI) and Writing Code",
    "text": "Artifical intelligence (AI) and Writing Code\nThere has been a recent explosion in the quality and availability of large language models (LLM), colloquially referred to simply as AI, of which ChatGPT remains the most famous. One of their much-vaunted features is the ability to write code, which begs the question: should I use AI to help me write my bioinformatics scripts? The answer is… it depends how you use it. LLMs will quickly churn out code which is (mostly) runnable. However, there is no guarantee that the code does exactly what you think it does. If you want to use AI-generated code, you will need to go through it line by line, checking and testing exactly what is happening; this, obviously, still requires you to understand the code in detail.\nOne area is which LLMs are likely to prove very useful is in debugging. Error messages usually contain useful information, but not necessarily in a wording accessible to a beginner. LLMs are showing promise for helping to identify and diagnose bugs in code.\nIf you want some tips for using LLMs to write code, this article from Nature provides some guidance."
  },
  {
    "objectID": "index.html#session-0.-basics-of-hpc-and-hpc-vs-personal-cloud-provision",
    "href": "index.html#session-0.-basics-of-hpc-and-hpc-vs-personal-cloud-provision",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 0. Basics of HPC and HPC vs Personal Cloud provision",
    "text": "Session 0. Basics of HPC and HPC vs Personal Cloud provision\nThis element of the course introduces you the the computational resources that have been provided for you to perform the training. It also covers a light guide to logging in to the research platforms at Cardiff School of Biosciences. If you are a researcher at Cardiff wanting to gain access to HPC please contact our Biocompute team and join the bioinformatics Teams community. If you are outside Cardiff - find out about your local HPC provision. If there is none locally look into cloud service such as AWS and Google …. many of these provide limited free options which can help you to learn."
  },
  {
    "objectID": "index.html#session-1.-linux-the-basics",
    "href": "index.html#session-1.-linux-the-basics",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 1. Linux: The basics",
    "text": "Session 1. Linux: The basics\nThis introduces you to the command line - yes, no more clicking on icons as it is all about writing commands !!\nIntroduction to Linux\nThis represent a rapid run through of the basics you need to get going on the command line, with lots of useful information and some basic exercises - copying / moving and learning about your Linux environment. This workshop is designed to get you started quickly.\nThis session gives more detail about the Linux basics, covering navigating around your Linux system (it has some graphical representations of your file system), auto-completion, file permissions and the fundamental anatomy of a linux command. We cover simple commands (copy/move ect.), how to preview files (less/cat/head/tail) and how to edit files (we use Nano but touch on using Vi).\nLoading Software and Accessing Data\nA guide to how to access the software and data you will need for the rest of the course, plus more useful Linux commands.\nExtension: Commandline Tools and Scripting\nFrom querying files with grep, running scripts and using loops, this material provides some more exercises to develop your skills."
  },
  {
    "objectID": "index.html#session-2.-ngs-data-manipulation-from-seqences-to-assemblies.",
    "href": "index.html#session-2.-ngs-data-manipulation-from-seqences-to-assemblies.",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 2. NGS Data Manipulation: From seqences to assemblies.",
    "text": "Session 2. NGS Data Manipulation: From seqences to assemblies.\nAn Introduction to NGS Data and Quality Control\nMost of our training material focuses on bioinformatics application in the area of genomics. This section introduces the fundamental concepts of Next Generation Sequencing (NGS) and the basic file types (fasta / fastq) and approaches to quality control and initial data processing.\nGenome Assembly\nApproaches to genome assembly vary hugely depending on what you are assembling and what type of data you have available to you. This course introduces the fundamental concepts of assembly and how you evaluate the quality of your assemblies. It provides examples and approaches that are good for small genomes - like organelles (15-200 kb) and bacterial genomes (1-5 Mb). The primary examples use short read (Illumina) have be customised to exploit long read (Nanopore / Pacbio) or combinations of short and long read data.\nExtension: Hybrid Assemblies\nOften the best way to get a high-quality genome is by a hybrid assembly: combining short-read (Illumina) and long-read (Nanopore/PacBio) data. This session will guide you through the principles of hybrid assembly."
  },
  {
    "objectID": "index.html#session-3.-genome-visualisation-and-annotation",
    "href": "index.html#session-3.-genome-visualisation-and-annotation",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 3. Genome visualisation and annotation",
    "text": "Session 3. Genome visualisation and annotation\nGenome Annotation and Visualisation\nA genome or transcript assembly means little until you overlay it with biological information. Here we introduce you to software to generate that biological information as well as visualize the results. We primarily use Artemis (Sanger Centre) but also introduce Integrated Genome Viewer (IGV) from the Broad Institute.\nExtension: Annotating Mitochodria\nAnnotating a mitochrondrial genome requires some different software to a bacterial genome. Here we walk you through using it."
  },
  {
    "objectID": "index.html#session-4.-building-phylogenies",
    "href": "index.html#session-4.-building-phylogenies",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 4. Building phylogenies",
    "text": "Session 4. Building phylogenies\nPhylogenetics and Phylogenomics\nThe taxonomic relationships between organisms can be derived through the genetic differences between them. This workshop provides a refresher of the basic principles and then provides examples of how to derive phylogenetic trees from single gene trees to whole genomes. Ultimately, the more global the information used to generate a phylogeny the more resolution / understanding you have about the relationship between organism at the level of the population or individual."
  },
  {
    "objectID": "index.html#session-5.-transcriptomics",
    "href": "index.html#session-5.-transcriptomics",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 5. Transcriptomics",
    "text": "Session 5. Transcriptomics\nRNAseq Data Processing\nTranscriptomics is all about counting - this workshop cover how we get from raw RNAseq reads to transcript counts for an organism with an existing genome. We include approaches to identify and evaluate technical duplication, although be aware this may not be relevant to your analytical approach.\nGeneration of Differentially Expressed Gene (DEG) Lists\nPerforming quality control and provisional data visualisation (Volcano plots and MA) are all essential steps on before generating differential gene lists. Here we use SARTools developed at the Pasteur Institute to illustrate best practice in transcriptome analysis. We also include multivariate approaches to data visualization such as PCA and HCA (hierarchical clustering analysis).\nFunctional Interpretation of DEGs\nInterpreting the biological significance of a list of gene IDs or gene symbols representing your DEGs can be a substantial but fun challenge. This course shows a selection of tools that allow you to go from DEG to functional networks."
  },
  {
    "objectID": "index.html#session-6.-microbial-community-analysis",
    "href": "index.html#session-6.-microbial-community-analysis",
    "title": "Introduction to Big Data Biology and Bioinformatics",
    "section": "Session 6. Microbial community analysis",
    "text": "Session 6. Microbial community analysis\nMetabarcoding\nAmplification of short, phylogenetically informative sections of DNA can be used to generate community profiles. This course introduces the concepts of metabarcoding the targets that are used for different phylogenetic groups (16S for bacteria, ITS for fungal, RbcL for algae and COI/18S for other eukaryotes). Data analysis uses (QIIME2)[https://qiime2.org/] to navigate you through preliminary metabarcoding data analysis."
  }
]