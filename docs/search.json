[
  {
    "objectID": "4.1_phylogentics_phylogenomics.html",
    "href": "4.1_phylogentics_phylogenomics.html",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "",
    "text": "Phylogenetics to phylogenomics Powerpoint\nOnline Phylogenetic Lecture"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#exercises-on-constructing-and-interpreting-phylogenies",
    "href": "4.1_phylogentics_phylogenomics.html#exercises-on-constructing-and-interpreting-phylogenies",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Exercises on constructing and interpreting phylogenies",
    "text": "Exercises on constructing and interpreting phylogenies\nBelow are four approaches for constrcting phylogenetic tress from a single gene, multiple genes, core genome gene, and whoel genome SNP phylogeny. Remember that we are processing the sequence data to obtain biological interpretations so think what question you are trying to answer. Don’t get lost in the terminal with typing commands meaninglessly, step back and think about the bioinformatic process to get to the end goal.\nWhich ever approach you are taking the overarching process follows the same principle steps:\n\nExtract (and if appropriate concatenate) the target(s) nucleotide sequence(s) from your samples. The approach will differ depending on if you constructing a single gene, multigene, pan genome or SNP approach. You should include a reference / outgroup sequence\nCreate an alignment of the nucleotide sequences from all samples. MAFFT is a commonly used linux aligner. Remember to ensure your sequences are trimmed and orientated in the same direction !!\nConstruct a phylogeny. RAxML-NG is a great very scalable phylogenetic program. You may wish to optimize the phylogenetic model you use - there are specific programs to do this but we will not cover these in this course.\nVisualize your tree Figtree is a good program for visualizing trees on linux platform, but you could equally download you trees and visualise using MEGAX."
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#single-gene-phylogeny-identify-the-unknown-cetacean",
    "href": "4.1_phylogentics_phylogenomics.html#single-gene-phylogeny-identify-the-unknown-cetacean",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Single gene phylogeny: Identify the unknown cetacean",
    "text": "Single gene phylogeny: Identify the unknown cetacean\nMany purposes you need to identify an species identify from which an assembly was derived. The most common way to do this is to construct a phylogeny based on a single gene. The gene used differs by kingdom. For bacteria and archaea people use 16S rRNA whilst for most Eukaryotas people use mitochondrial COI (Cytochrome Oxidase I). There are a few domain specific loci used, for instance Algal people use large subunit of ribulose bisphosphate carboxylase (RbcL) whilst the fungal communities use internal transcribed spacer (ITS) between there nuclear ribosomal genes.\n\nIf a outgroup is not provided use NCBI Genbank or the European Nucleotide Archive to download an appropriate outgroup (remember an outgroup should be the same phylogenetic marker (in the correct orientation) from a species that is phylogenetically distinct from but still related to your study group. i.e. If you are studying dog species use a CoI from a Cat.\nAnnotate your assembled contigs and extract out your phylogenetic marker.\n\n\nIf using prokayotic geneomes use Prokka or Barrnap to annotate your assembled genome generating a gff file. If you have a mtDNA then use Mitos to perform the annotation.\nUsing bedtools to extract a sequence file (e.g. 16S or CoI) from the original assembly.\n\n\nUse MAFFT to create an alignment of your phylogenetic marker.\nConstruct a phylogeny with RAxML-NG\nVisualize your tree Figtree"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#muliple-gene-plylogenys-mlst",
    "href": "4.1_phylogentics_phylogenomics.html#muliple-gene-plylogenys-mlst",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Muliple gene plylogenys: MLST",
    "text": "Muliple gene plylogenys: MLST\nThe most common use of multiple genes to create phylogenogies is Multi-locus sequence typing (MLST). This is primarily used in bacterial phylogenetics when it is necessary to go beyond species and identify the strain. This method is extreme valuable when identifying pathogenic strains away for sometimes harmless relatives. Established ‘Stereotype’ have often now been align with specific MLST allele sequences. There are on-line tools that allow you to upload your sequence and perform MLST analysis - such as autoMLST. However, many of the resources are ‘species specific’ and allowing you to enter a draft genome from specific species and using MLST appropriate to plylogenetically ‘type’ the strain it represents. Many of these tools can be accessed through pubmlst. There are also a myriad of command line tools that can be used - simplest to remember and a very easy one to use is the software entitled MLST.\nRemember you can only use these tools if you have already identified the species and if there is a MLTS database for the organism in question.\nThis approach is very rarely used for Eukaryotes."
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#core-gene-phylogeny-understand-the-diversity-of-a-bacterial-species",
    "href": "4.1_phylogentics_phylogenomics.html#core-gene-phylogeny-understand-the-diversity-of-a-bacterial-species",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Core gene phylogeny: Understand the diversity of a bacterial species",
    "text": "Core gene phylogeny: Understand the diversity of a bacterial species\nOrganisms within particular phylogenetic groups share a set of ‘core’ genes - these are often those representing essential metabolism. Then each species has a unique set of ‘ancillary’ genes which define the unique phenotype of the organism. Worth noting that the ultimate bacterial ‘ancillary’ genes are mobile genetic elements that carry things such as antibiotic resistance genes. For bacteria this ‘core’ and ‘ancillary’ relationship is well define and programs have been developed that identify the ‘core’ genome by identifying all the ‘shared’ gene loci between a group of bacteria, concatenate these sequences and use them to derive create a phylogeny. The program we will use to demonstrate this is panaroo which will identify, extract, and concatenate the core genome.\nSimilar approach can be taken with Eukaryotic genomes. A project know as Busco has been working to identify list of core genes within different phylogenetic lineages. A tool and database allows you to identify and extract those core genes from draft genomes and even has plugin for generating phylogenies.\nThe procedure to create a bacterial pan-genome phylogeny is:\n\nIdentify a suitable outgroup for this phylogeny (closely related species) – look back at the lecture to understand outgrouping in core genome phylogenies. Download a suitable genome from NCBI Genbank or the European Nucleotide Archive and upload to your virtual machine.\nAnnotate your bacterial genomes using Prokka.\nPredict and align the pangenome and construct a core gene alignment by processing the gff annotation files with panaroo using MAFFT alignment option.\nConstruct a phylogeny with RAxML-NG.\nVisualize your tree Figtree\n\nIf you were to repeat this analysis without the designated outgroup, which strain/s would be the outgroup of the phylogeny? (check lecture for guidance)."
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#whole-genome-snp-phylogeny",
    "href": "4.1_phylogentics_phylogenomics.html#whole-genome-snp-phylogeny",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Whole genome SNP phylogeny",
    "text": "Whole genome SNP phylogeny\nTracking individual mutations between strains of an organism allows a precision in the phylogenetic analysis unlock phenomenal potential - since these mutation or SNP occur and a organism is transmitted between individual we can use this within a pathogenic context, viral or bacterial, to explore epidemiology of disease.\nUsing whole mitochondrial SNP data we can explore relationship between genetically distinct ‘breeds’ of the same organism or isolated populations to derive conservation requirements or determine relationships between ancient ancestral populations (often extinct).\nOne significant advantage of performing a SNP phylogeny is it can be performed with the trim quality control reads and does NOT require de novo assembly. The limitation is that a good and complete reference is required. The process aligns the reads to the reference, derives SNPs, aligns these SNPs from different samples and uses this to generate a phyogeny.\n\nIdentify a reference genome. Either use the sequenec provided or use NCBI Genbank or the European Nucleotide Archive online to download a suitable outgroup/reference genome and upload to your virtual machine.\nRecall from previosu session that we processed raw sequence data prior to use. Use Fastqc and Fastp to quality check and remove adaptors/trim poor quality bases, respectively.\nUse snippy to identify SNPs between the reference/outgroup genome and both your processed unknown samples and processed characterised sequence data - you need to analyse both datasets (characterised and unknown) otherwise your phylogeny will look pretty sparse!\nUse snippy-core to create an alignment of SNPs.\nUse snp-sites to align the SNPs\nConstruct a phylogeny with RAxML-NG.\nVisualize your tree Figtree"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#sequence-data-available-for-the-phylogeny-exercises",
    "href": "4.1_phylogentics_phylogenomics.html#sequence-data-available-for-the-phylogeny-exercises",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Sequence data available for the phylogeny exercises",
    "text": "Sequence data available for the phylogeny exercises\n\nSingle Gene Phylogeny\n\nAssembled and annoated prokka prokayotic genomes\nAssembled and annotated mitos mitochondrial genomes\n\n\n\nCore Gene Phylogeny\n\nAssembled and annoated prokka prokayotic genomes\n\n\n\nSNP Phylogeny\n\nAssembled and annoated prokka prokayotic genomes\nAssembled and annotated mitos mitochondrial genomes"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#bioinformatic-software-and-tools",
    "href": "4.1_phylogentics_phylogenomics.html#bioinformatic-software-and-tools",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Bioinformatic software and tools",
    "text": "Bioinformatic software and tools\nWe will use multiple bioinformatic packages to extract and process the sequence data to construct phylogenies\nProkka – genome annotation\nbedtools – manipulate and extract sequences from fasta files\nMAFFT – sequence alignment\nRAxML-NG – construct phylogenies\npanaroo – pangenome pipeline\nsnippy – identify sequence variants snp-sites - align snp sites\n\nRemember that you can access the “help” option for almost all bioinformatics tools by executing the name of the tool with no flags/options, or adding -h or –help. After using a program, dont forget to unload the module."
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#software-usage",
    "href": "4.1_phylogentics_phylogenomics.html#software-usage",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Software usage",
    "text": "Software usage\n\nProkka for bacterial genome annoation\nWhole genome annotation is the process of identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.\n\nmodule load prokka_mambaforge/1.14.6\n\nBasic usage:\n\nprokka --prefix [output file prefix] --outdir [output directory] --cpus [cpu number] --kingdom Bacteria [fasta file]\n\nProkka outputs the annotation in multiple formats. Open the gbk and gff files to see their structure. Both are annotation files that indicate the location of genes/proteins, their sequence, and their putative function based on sequence comparisons to databases\n\n\n\nBarrnap\nBAsic Rapid Ribosomal RNA Predictor. Can be used to annotate rRNA genes in genomes.\n\nmodule load barrnap/0.8\nmodule load hmmer/3.3.2\n\n\nBasic usage:\n\nbarrnap --kingdom bac --threads [number of cpus] [fasta file] &gt; [output gff annotation file]\n\n\n\nmitos for mitochondrial annotation\n\nmodule load mitos/2.0.4\nmodule load seqtk/1.3\n\n\n\n# make output dir\nmkdir mitos_output/[sample name]\n\n#merge secondary contigs - this artificially makes you assembly \nsed -i '/^&gt;[2-9].*$/d' [assembly.fasta]\n\n#remove line breaks\nseqtk seq -l 0 [assembly.fasta]\n\nrunmitos.py  -i [assembly.fasta] -c 2 -o [dir name for sample annotation] -R ~/classdata/Bioinformatics/REFS/mitos -r refseq81m --rrna 0 --trna 0 --intron 0 --debug --noplot\n\n\n\nBedtools\nCollectively, the bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line.\n\nmodule load bedtools2/2.30.0\n\nWe can use bedtools to extract gene/protein sequences from annotation files:\n\nNavigate to either the Prokka annotation output directory or the Barrnap output file\nCreate a sub-gff file that contains the annotation information of only the 16 rRNA gene or CoI gene\n\n\n# For bacetrial sequences\ngrep \"product=16S ribosomal RNA\" [gff annotation file]  &gt; [subset_16S.gff]\n\n# For mitochondrial sequences\ngrep \"ID=gene_cox1\" [gff annotation file]  &gt; [subset_coi.gff]\n\n\nExtract the 16S rRNA gene from the mitochondrial genome assembly using the subset_16S.gff annotation file (contains the co-ordinates for the start/stop positions). Depending on how the assembly graph was made, the sequence may be in the opposite orientation (reverse complement). To account for this we need to extract based on the strandedness using the -s option.\n\n\nbedtools getfasta -fi [input fasta file] -bed [subset_gff file] -s &gt; [output 16S file.fasta]\n\nThe fasta sequence will be given a numeric non-sensicle name (what coes after the &gt;) - what about changing this for the name of the source sequence.\n\nsed -i \"s/&gt;.*/&gt;[sequence name]/g\" [output 16S file.fasta]\n\n\nMove your extracted 16S rRNA gene sequence into the directory of 16S rRNA reference genes provided.\nCombine all the 16S rRNA gene fasta files into a single multifasta file\n\n\ncat [*.fasta] &gt; [output multifasta file]\n\n\n\nMAFFT\nMAFFT is a multiple sequence alignment program for unix-like operating systems.\n\nmodule load mafft/7.481\n\nBasic usage:\n\nmafft [input multifasta] &gt; [output alignment multifasta]\n\n\n\n\nRAxML-NG\nRAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML) optimality criterion.\n\nmodule load raxml-ng/1.0.2\n\nRAxML-NG outputs files to your current location. Create a directory and run RAxML from the directory to keep files organised\nBasic usage for nucleotide alignment with GTR model:\n\nraxml-ng-mpi --all --msa [input alignment] --model GTR --bs-tree [bootstrap replicate number e.g. 100] --prefix [prefix for output files] --threads [number of threads/cpus]\n\nRAxML-NG produces multiple output files. The file we are interested in has the extension .support\nAdd the extension .nwk to this file to indicate it is a phylogeny.\n\n\npanaroo\nA Bacterial Pangenome Analysis Pipeline. Predicts core, accessory, and pangenomes. Produces core gene alignments.\n\nmodule load panaroo/1.3.0\n\n\nPanaroo takes annotation files in .gff format to predict pangenomes.\nUse prokka to annotate the Bacillus velezensis genomes supplied, and remember to change the kingdom flag to bacteria\nBasic usage:\n\npanaroo -i *gff -o [output directory] -t [number of cpus] --clean-mode sensitive -a core --aligner mafft\n\nOutput files of interest include the summary_statistics.txt and core_gene.aln\n\nCheck the website for details of all the files produced: https://gtonkinhill.github.io/panaroo/#/gettingstarted/output\n\n\nsnippy\nA tool to identify variations between a reference genome in fasta format and genome sequence data in read format (fastq)\n\nmodule load snippy/v4.6.0\nmodule load samtools/1.15.1\n\n\nBasic usage:\nTo identify SNPs for one genome\n\nsnippy -outdir [snippy output directory/sampleID] -ref [reference sequence fasta] -R1 [sample1 fastq left trimmed read] -R2 [sample2 fastq right trimmed read] --prefix [prefix for output] --force\n\nYou will need a loop to call SNPs for multiple genomes.\n\nTo produce an alignment from SNPs, navigate to the snippy output directory and run snippy-core. The wildcard will extract the necessary SNP data from all the genomes analysed by snippy (directory for each genome in output directory).\n\nsnippy-core -ref [same reference sequence] -prefix [prefix for snp output] [snippy output directory]/*\n\nGenerate an alignment of the SNP sites:\n\nmodule load snp-sites/2.5.1\n\n\nsnp-sites -cb -o [alniment file name] [prefix for snp output].full.aln\n\n\n\n\nFigtree\nFigTree is designed as a graphical viewer of phylogenetic trees and as a program for producing publication-ready figures.\nDownload locally to your laptop: https://github.com/rambaut/figtree/releases / or access through Guacamole graphical user interface"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#single-gene",
    "href": "4.1_phylogentics_phylogenomics.html#single-gene",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Single Gene",
    "text": "Single Gene"
  },
  {
    "objectID": "4.1_phylogentics_phylogenomics.html#whole-genome-snp",
    "href": "4.1_phylogentics_phylogenomics.html#whole-genome-snp",
    "title": "Phylogenomics and Phylogeneomics",
    "section": "Whole Genome SNP",
    "text": "Whole Genome SNP"
  },
  {
    "objectID": "5.1_Transcriptomics_Introduction.html",
    "href": "5.1_Transcriptomics_Introduction.html",
    "title": "Transcritomics An Introduction",
    "section": "",
    "text": "Transcripromics Data: An Introduction powerpoint"
  },
  {
    "objectID": "5.1_Transcriptomics_Introduction.html#transcripromics-data-an-introduction",
    "href": "5.1_Transcriptomics_Introduction.html#transcripromics-data-an-introduction",
    "title": "Transcritomics An Introduction",
    "section": "",
    "text": "Transcripromics Data: An Introduction powerpoint"
  },
  {
    "objectID": "5.1_Transcriptomics_Introduction.html#deriving-differential-gene-expression",
    "href": "5.1_Transcriptomics_Introduction.html#deriving-differential-gene-expression",
    "title": "Transcritomics An Introduction",
    "section": "Deriving Differential Gene Expression",
    "text": "Deriving Differential Gene Expression\nDeriving Differential Gene Expression"
  },
  {
    "objectID": "5.2_RNAseq_processing.html",
    "href": "5.2_RNAseq_processing.html",
    "title": "RNAseq Processing",
    "section": "",
    "text": "RNAseq Processing Powerpoint"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#alignment",
    "href": "5.2_RNAseq_processing.html#alignment",
    "title": "RNAseq Processing",
    "section": "Alignment",
    "text": "Alignment\nSTAR – (Spliced Transcript Alignments to a Reference) is an alignment package which functions similarly to standard genome alignments but is designed for short regions of RNA that could span intron-exon junctions and with low compute requirements. STAR outputs a bam format file which contains the locations where all the reads in your dataset have aligned and the genes they cover."
  },
  {
    "objectID": "5.2_RNAseq_processing.html#counting",
    "href": "5.2_RNAseq_processing.html#counting",
    "title": "RNAseq Processing",
    "section": "Counting",
    "text": "Counting\nFeatureCounts is a simple package that takes the positions of mapped reads and outputs a file quantifying the expression of each gene or exon (based on parameter choices). At this point raw read counts are hard to interpret due to likely different levels of sequencing achieved per sample and methodological biases.\nOne common step prior to counting is marking duplicates that arise from data generation for further information, or so that they can be removed. Here we’ll use the imaginatively named MarkDuplicates from GATK."
  },
  {
    "objectID": "5.2_RNAseq_processing.html#differential-gene-analysis",
    "href": "5.2_RNAseq_processing.html#differential-gene-analysis",
    "title": "RNAseq Processing",
    "section": "Differential Gene Analysis",
    "text": "Differential Gene Analysis\nContrasting the expression profile of the samples is typically done with one of two R packages: Deseq2 or EdgeR (the mac vs windows of the RNAseq fight), however a multitude of alternatives exist. These packages perform the normalization and statistical steps of contrasting samples as defined in a metadata file stating your experimental design (replicates, tissue type, treatment etc). The output here is a range of significant genes, ordinance and cluster analysis of sample similarity, and various quality control figures.\nFollowing these three steps, there are an almost infinite number of tools and packages to look deeper into your data, find experimentally specific insights, and prior published data to contrast against."
  },
  {
    "objectID": "5.2_RNAseq_processing.html#data",
    "href": "5.2_RNAseq_processing.html#data",
    "title": "RNAseq Processing",
    "section": "Data",
    "text": "Data\nThe data you will need for this exercise are:\n~/classdata/Bioinformatics/Day4/ RNAseq-Processing/fastq\nSRR5222797_1.fastq    SRR5222797_2.fastq\nSRR5222798_1.fastq    SRR5222798_2.fastq\nSRR5222799_1.fastq    SRR5222799_2.fastq\n\n~/classdata/Bioinformatics/STAR/REFS\nArabidopsis_thaliana.TAIR10.47.gtf\nArabidopsis_thaliana.TAIR10.dna_sm.toplevel.fa\nThis folder contains lots of other index files for star to function that you don’t need to touch! Note: most programs will accept fastq or fastq.gz without any changes however star requires you to include the --readFilesCommand zcat parameter."
  },
  {
    "objectID": "5.2_RNAseq_processing.html#software",
    "href": "5.2_RNAseq_processing.html#software",
    "title": "RNAseq Processing",
    "section": "Software",
    "text": "Software\nWe will be using scripts to run these steps. In the classdata/Day4/scripts folder you will find the following that you can use to base your analysis, however make sure you’re tuning it to your own file structure and file names.\nSo far we have used only a small dataset to quickly practice the steps but now we’ll be using a full sized RNAseq sample otherwise it causes the programs to think it’s bad data. In the classdata/Day3 folder there four pairs of RNAseq files from an Arabidopsis RNAseq study. In the folder classdata/REFS there is a reference genome, and a gtf file. The step 2 “star index genome” has already been ran for you (you don’t need to do this!)\n~/classdata/Bioinformatics/Day4/RNAseq-Processing\nScripts\n\n1-QC.sh\n2-star_index_genome.sh (already done, don’t repeat!)\n3-star.sh\n\n4-markduplicates.sh\n\n5-featurecounts.sh"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#qc.sh",
    "href": "5.2_RNAseq_processing.html#qc.sh",
    "title": "RNAseq Processing",
    "section": "1-QC.sh",
    "text": "1-QC.sh\n\n#!/bin/bash\n\n## Load some Modules\nmodule load fastqc/0.11.9\nmodule load trimmomatic/0.39\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\nfor i in ${list[@]}\ndo\n## The commands you want to run\n\n# fastqc the raw data\nfastqc -t 4 $workingdir/fastq/${i}_1.fastq\nfastqc -t 4 $workingdir/fastq/${i}_2.fastq\n\n# Run trimmomatic\ntrimmomatic PE $workingdir/fastq/${i}_1.fastq $workingdir/fastq/${i}_2.fastq  -baseout $workingdir/fastq/${i}-trim.fastq ILLUMINACLIP:/mnt/clusters/sponsa/data/classdata/Bioinformatics/REFS/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15\n\n# fastqc the outputs\nfastqc -t 4 $workingdir/fastq/${i}-trim_1P.fastq\nfastqc -t 4 $workingdir/fastq/${i}-trim_2P.fastq\n\ndone"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#star_index_genome.sh",
    "href": "5.2_RNAseq_processing.html#star_index_genome.sh",
    "title": "RNAseq Processing",
    "section": "2-star_index_genome.sh",
    "text": "2-star_index_genome.sh\n#!/bin/bash\n\n# Load some modules\nmodule load star/2.7.6a\n \n## Useful shortcuts\nexport refdir=~/classdata/REFS\n\n## Change --sjdbOverhang to length of your sequence data /2 minus 1\n\n\necho \"\\n\\n I TOLD YOU NOT TO RUN THIS ONE NOW! \\n\\n (unless you're in the future and trying to run this for real, in which case you need to edit this script and remove the # characters from the command)\"\n\nSTAR    --runThreadN 8 \\\n        --limitGenomeGenerateRAM 321563573 \\\n        --runMode genomeGenerate \\\n        --genomeDir  $refdir/ \\\n        --genomeFastaFiles $refdir/Arabidopsis_thaliana.TAIR10.dna_sm.toplevel.fa \\\n        --sjdbGTFfile $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        --sjdbOverhang 49"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#star.sh",
    "href": "5.2_RNAseq_processing.html#star.sh",
    "title": "RNAseq Processing",
    "section": "3-star.sh",
    "text": "3-star.sh\n#!/bin/bash\n\n## Load some Modules\nmodule load star/2.7.6a\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\nexport refdir=~/classdata/Bioinformatics/REFS/STAR/\n\n## The commands you want to run\nmkdir $workingdir/star\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\nfor i in ${list[@]}\ndo\n# map forward and reverse reads to genome\n# If input data is gzipped (.fastq.gz) inculde the additional parameter:   --readFilesCommand zcat\nSTAR   --outMultimapperOrder Random \\\n       --outSAMmultNmax 1 \\\n       --runThreadN 4  \\\n       --runMode alignReads \\\n       --outSAMtype BAM Unsorted \\\n       --quantMode GeneCounts \\\n       --outFileNamePrefix $workingdir/star/${i}-unsort. \\\n       --genomeDir $refdir \\\n       --readFilesIn $workingdir/fastq/${i}-trim_1P.fastq $workingdir/fastq/${i}-trim_2P.fastq\ndone"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#markduplicates.sh",
    "href": "5.2_RNAseq_processing.html#markduplicates.sh",
    "title": "RNAseq Processing",
    "section": "4-markduplicates.sh",
    "text": "4-markduplicates.sh\n#!/bin/bash\n\n#load some modules\nmodule load picard/2.26.2\nmodule load samtools/1.15.1\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\n\nmkdir markdup\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\nfor i in ${list[@]}\ndo\nsamtools index $workingdir/star/${i}-unsort.Aligned.out.bam\nsamtools sort -@ 4 -o $workingdir/star/${i}.sorted.bam $workingdir/star/${i}-unsort.Aligned.out.bam\n\n##  MARK DUPLICATES  ##\npicard MarkDuplicates I=$workingdir/star/${i}.sorted.bam O=$workingdir/markdup/${i}.markdup.bam M=$workingdir/markdup/${i}.metrics.markdup.txt REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT\n\n\n## REMOVE DUPLICATES ##\npicard MarkDuplicates I=$workingdir/star/${i}.sorted.bam O=$workingdir/markdup/${i}.rmdup.bam M=$workingdir/markdup/${i}.metrics.rmdup.txt REMOVE_DUPLICATES=true VALIDATION_STRINGENCY=SILENT\n\ndone"
  },
  {
    "objectID": "5.2_RNAseq_processing.html#featurecounts.sh",
    "href": "5.2_RNAseq_processing.html#featurecounts.sh",
    "title": "RNAseq Processing",
    "section": "5-featurecounts.sh",
    "text": "5-featurecounts.sh\n#!/bin/bash\n\n# Load some modules\nmodule load subread/2.0.2\n\n## Useful shortcuts\nexport workingdir=~/mydata/Session5/RNAseq-Processing\nexport refdir=~/classdata/Bioinformatics/REFS/STAR/\n\n\nmkdir $workingdir/featureCounts\n\n#list=(\"sample1\" \"sample2\" \"sample3\")\nlist=(\"SRR5222797\" \"SRR5222798\" \"SRR5222799\")\n\n\n\nfor i in ${list[@]}\ndo\n\nfeatureCounts \\\n        -T 4 -p -F GTF -t exon -g gene_id \\\n        -a $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        -o $workingdir/featureCounts/${i}.markdup.featurecount \\\n        $workingdir/markdup/${i}.markdup.bam\n\nfeatureCounts \\\n        -T 4 -p -F GTF -t exon -g gene_id \\\n        -a $refdir/Arabidopsis_thaliana.TAIR10.47.gtf \\\n        -o $workingdir/featureCounts/${i}.rmdup.featurecount \\\n        $workingdir/markdup/${i}.rmdup.bam\n\ndone"
  },
  {
    "objectID": "5.3_DEG_analysis.html",
    "href": "5.3_DEG_analysis.html",
    "title": "Differential Gene Expression Analysis",
    "section": "",
    "text": "DEG Analysis Powerpoint"
  },
  {
    "objectID": "5.3_DEG_analysis.html#alignment",
    "href": "5.3_DEG_analysis.html#alignment",
    "title": "Differential Gene Expression Analysis",
    "section": "Alignment",
    "text": "Alignment\nSTAR – (Spliced Transcript Alignments to a Reference) is a pseudo-alignment package which functions similarly to standard genome alignments but is designed for short regions of RNA that could span intron-exon junctions and with low compute requirements. STAR outputs a bam format file which contains the locations where all the reads in your dataset have aligned and the genes they cover."
  },
  {
    "objectID": "5.3_DEG_analysis.html#counting",
    "href": "5.3_DEG_analysis.html#counting",
    "title": "Differential Gene Expression Analysis",
    "section": "Counting",
    "text": "Counting\nFeatureCounts is a simple package that takes the positions of mapped reads and outputs a file quantifying the expression of each gene or exon (based on parameter choices). At this point raw read counts are hard to interpret due to likely different levels of sequencing achieved per sample and methodological biases."
  },
  {
    "objectID": "5.3_DEG_analysis.html#differential-gene-analysis",
    "href": "5.3_DEG_analysis.html#differential-gene-analysis",
    "title": "Differential Gene Expression Analysis",
    "section": "Differential Gene Analysis",
    "text": "Differential Gene Analysis\nContrasting the expression profile of the samples is typically done with one of two R packages: Deseq2 or EdgeR (the mac vs windows of the RNAseq fight), however a multitude of alternatives exist. These packages perform the normalization and statistical steps of contrasting samples as defined in a metadata file stating your experimental design (replicates, tissue type, treatment etc). The output here is a range of significant genes, ordinance and cluster analysis of sample similarity, and various quality control figures.\nFollowing these three steps, there are an almost infinite number of tools and packages to look deeper into your data, find experimentally specific insights, and prior published data to contrast against."
  },
  {
    "objectID": "5.3_DEG_analysis.html#converting-between-common-gene-ids",
    "href": "5.3_DEG_analysis.html#converting-between-common-gene-ids",
    "title": "Differential Gene Expression Analysis",
    "section": "Converting between common gene IDs",
    "text": "Converting between common gene IDs\n\nbiodbnet\ngprofiler convert"
  },
  {
    "objectID": "5.3_DEG_analysis.html#whole-dataset-annotation-and-ontologies",
    "href": "5.3_DEG_analysis.html#whole-dataset-annotation-and-ontologies",
    "title": "Differential Gene Expression Analysis",
    "section": "Whole dataset annotation (and ontologies)",
    "text": "Whole dataset annotation (and ontologies)\n\ngProfiler\nKEGG\nNeVOmics"
  },
  {
    "objectID": "5.3_DEG_analysis.html#interaction-networks",
    "href": "5.3_DEG_analysis.html#interaction-networks",
    "title": "Differential Gene Expression Analysis",
    "section": "Interaction Networks",
    "text": "Interaction Networks\n\nStringDB\nGOnet\nCytoscape\n\nbingo plugin\nEnrichmentMap plugin"
  },
  {
    "objectID": "5.3_DEG_analysis.html#other-visualisation-tools",
    "href": "5.3_DEG_analysis.html#other-visualisation-tools",
    "title": "Differential Gene Expression Analysis",
    "section": "Other visualisation tools",
    "text": "Other visualisation tools\n\nMorpheus\nComplex venn"
  },
  {
    "objectID": "5.3_DEG_analysis.html#whole-packages",
    "href": "5.3_DEG_analysis.html#whole-packages",
    "title": "Differential Gene Expression Analysis",
    "section": "Whole packages",
    "text": "Whole packages\n\nIPA\nShiny-Seq\nBeavR"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html",
    "href": "5.4_DEG_Functional_Interpretation.html",
    "title": "DEG Functional Interpretation",
    "section": "",
    "text": "Workshop Recording to come"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#processing-rnaseq-data",
    "href": "5.4_DEG_Functional_Interpretation.html#processing-rnaseq-data",
    "title": "DEG Functional Interpretation",
    "section": "Processing RNAseq Data",
    "text": "Processing RNAseq Data\nConverting RNASeq data into gene counts is the first-step prior to analysis of the biological function and networks revealed through the subsequent transcription analysis. This process is outside the scope of this workshop but if you are interested Andres et al 2013 Provides an excellent overview of the processes involved.\n Figure 1. Process overview"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#dataset-to-use---geogds2565",
    "href": "5.4_DEG_Functional_Interpretation.html#dataset-to-use---geogds2565",
    "title": "DEG Functional Interpretation",
    "section": "Dataset to use - Geo:GDS2565",
    "text": "Dataset to use - Geo:GDS2565"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#panopto_walk_through",
    "href": "5.4_DEG_Functional_Interpretation.html#panopto_walk_through",
    "title": "DEG Functional Interpretation",
    "section": "Panopto_walk_through",
    "text": "Panopto_walk_through"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#false-discover-rate",
    "href": "5.4_DEG_Functional_Interpretation.html#false-discover-rate",
    "title": "DEG Functional Interpretation",
    "section": "False Discover Rate",
    "text": "False Discover Rate"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#fishers-exact-test-and-enrichment-analysis",
    "href": "5.4_DEG_Functional_Interpretation.html#fishers-exact-test-and-enrichment-analysis",
    "title": "DEG Functional Interpretation",
    "section": "Fisher’s Exact Test and Enrichment Analysis",
    "text": "Fisher’s Exact Test and Enrichment Analysis"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#geo-workshop",
    "href": "5.4_DEG_Functional_Interpretation.html#geo-workshop",
    "title": "DEG Functional Interpretation",
    "section": "Geo Workshop",
    "text": "Geo Workshop\n\nSearch for you dataset\n\nGo to Geo DataSets: (https://www.ncbi.nlm.nih.gov/gds/)[https://www.ncbi.nlm.nih.gov/gds/]\n GEO_DataSet Search\nnow select \n\nRefine search to only show ‘DataSets’ and ‘Series’\n\nSelect &lt; DataSets and Series &gt; from the left hand menu.\n Refine to DataSets\n3A. Select procesed dataset (should be number 3 in list and have a heat map icon to the right)\nClick on title &lt; Endothelial cell response to ultrafine particles &gt; to select DataSet\n Entry Page\nTake notes off all pertinent information about the experiment, including species and what microarray platform was used. In this case experiment was conducted on rats and analysed on an Affymetrix Human Genome U133 Plus 2.0 Array. Download any publication available. Also, if you are interested you could have a look at the cluster analysis on the right hand site. This will show you how the relationship of the expression profiles from each sample relative to each other. If the experiment was successful, all samples of a certain treatment should cluster together.\n4A. Compare experiment samples\nClick &lt;Compare 2 sets of sample&gt;\nChoose &lt;test e.g. One-tailed t-test (A &gt; B)&gt;\nChoose _ e.g. 0.001\nClick on: Step 2: Select which Samples to put in Group A and Group B\n Select Groups to compare\nChoose &lt; Query Group A vs B &gt;\nYou should now see a list of the following DEGS\n DEG List\n5A. Download DEGs\nYou have &gt;100 DEGs but the page only displays the first 20. Before downloading DEGS change the items per page from 20 to 500.\n Items Per Page\nSelect &lt; Download profile data &gt;\nand you will be prompted to save the profiling of the DEGs displayed as default file name &lt;profile_data.txt&gt;\nSave to appropriate location. If needed Select Page 2…. of the DEGS and repeat the download process.\nRepeat this process for &lt;test e.g. One-tailed t-test (B &gt; A) significance level 0.001 &gt;_\n6A. Open and Merge DEG lists in Excel\nThe DEG lists show the gene list with there relative expression level (normalised) and annotation for the genes involved (annotation shown in columns BG -&gt;). For Our next steps we will use the Gene symbol that is in Column BH.\n3B. Select un-processed series (should be number 2 - it will have icon &lt; Analyze with GEO2R &gt; at the end of the entry)\nClick on &lt; Analyze with GEO2R &gt;\nNow Define groups by click clicking the &lt; Define Group pull &gt; down and create groups ‘control’ and ‘Treatment’ (enter group name and press enter). Click on each sample in list and associated it with one of your group (you can hold ctrl down to select multiple entry before associating them with the group).\n Select Groups to compare\n4B. Customise the Option and Analyse\nSelect &lt; Options &gt; and customise as shown below:\n Geo2R Options\nSelect &lt; reanalyze &gt;\nWill will now see a Processing icon - this may take a minute or two.\n5B. DEGs\nYou will now see a table and a series of Visualisations - review the visualisation taking note of what each are showing you.\n Geo2R results\nVenn diagram showing GSE4567: Limma, Padj&lt;0.05 - 704 genes - this is the set we will use\nClick on &lt; Explore and download, control vs treatment and Download Significant genes &gt;\nThis will give you a Tsv you can open in excel\n6B DEG TSV\nOpen the DEG TSV - you will have ID (Affymetrix), Gene Symbol, Description and Log2(fold Change) and Adjusted p-value. The Gene.symbol can have multiple symonyms for each gene - this can bias future analsyis. Copy/paste gene symbol column to rught hand column (so no other data is on right), and use &lt; Data &gt; Text to Columns &gt; Delimited &gt; Other &gt; ‘/’ &gt; Finish &gt;_ to push secondary symbols into other columns."
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#geo-database-panopto",
    "href": "5.4_DEG_Functional_Interpretation.html#geo-database-panopto",
    "title": "DEG Functional Interpretation",
    "section": "GEO Database Panopto",
    "text": "GEO Database Panopto"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#gprofiler-panopto",
    "href": "5.4_DEG_Functional_Interpretation.html#gprofiler-panopto",
    "title": "DEG Functional Interpretation",
    "section": "gprofiler panopto",
    "text": "gprofiler panopto"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#gene-enrichment-analysis-with-david",
    "href": "5.4_DEG_Functional_Interpretation.html#gene-enrichment-analysis-with-david",
    "title": "DEG Functional Interpretation",
    "section": "Gene Enrichment Analysis with David",
    "text": "Gene Enrichment Analysis with David\n[David - Database for Annotation, Visualization and Integrated Discovery] (https://david.ncifcrf.gov/) was the tool to use for gene enrichment from 2005 - 2016 but the database it hosted became out of date due to a break in the research groups funding. From 2016 (gprofiler)[https://biit.cs.ut.ee/gprofiler/gost] and (stringdb)[https://string-db.org/] have been prefered because they have been kept upto date. However, DAVID was refunded and a 2021 update means it is back to being maintained. Although there will not be time to explore it in our workshop the following workshop and video is there to support anyone wanting to explore its functionality."
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#start-analysis-and-upload-you-data",
    "href": "5.4_DEG_Functional_Interpretation.html#start-analysis-and-upload-you-data",
    "title": "DEG Functional Interpretation",
    "section": "1. Start analysis and upload you data",
    "text": "1. Start analysis and upload you data\nSelect &lt; start analysis &gt;\nPaste in you gene list (1) in box (A) and, select identifier (2), and identify as a Gene List (3). Select your species (2a) . In more nuanced analysis you may wish to define your own background - this is useful when working with non-model organisms or if your starting population does not represent the entire genome.\n DAVID Upload\nnow select &lt; Submit List &gt;"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#generate-functional-analysis-of-gene-list",
    "href": "5.4_DEG_Functional_Interpretation.html#generate-functional-analysis-of-gene-list",
    "title": "DEG Functional Interpretation",
    "section": "2. Generate Functional Analysis of gene list",
    "text": "2. Generate Functional Analysis of gene list\nIf you have used a non-regulate gene identifier or it does not recognize the identifier type you have used it may ask you to convert the identifiers - check the programs suggestion - it is usually good but you should check.\nSelect &lt; Functional Annotation Tool &gt;\nYou will now be give a annotation summary as shown below:\n Annotation Summary\nYou will see a range of categories for which the functional annotation has been performed. The analysis told has not only cross referenced the gene list against a range of functional databases it has also analysed the gene co-appearance in citations (Literature), links to disease (Disease) and Interactions agonist other linkages."
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#review-gene-ontology-enrichment",
    "href": "5.4_DEG_Functional_Interpretation.html#review-gene-ontology-enrichment",
    "title": "DEG Functional Interpretation",
    "section": "3. Review Gene Ontology Enrichment",
    "text": "3. Review Gene Ontology Enrichment\nLet us consider the functional enrichment using (Gene Ontology)[http://geneontology.org/]. Gene Ontology categorizes gene products using three distinct characteristics - Molecular Function (biochemistry of the product), Cellular Component (where it appears in the cell), and the Biological Processes - see more about these classification by reviewing the (Gene Ontology Overview documentation)[http://geneontology.org/docs/ontology-documentation/]. Although ontologies are hierarchical, they are not a simple classification system, they not only allow for gene products to be involved with multiple processes the relationship between elements in the hierarchy are closely defined. The ontology also allows for classification to be as specific or detailed as knowledge allows ie if a protein is a transporter but what it transports is not known it will be defined by a mid-level term ‘transporter’ but if it is known to transport Zn it will be classified at a ‘Zinc transport’ as well as a ‘transporter’. Enrichment analysis uses fisher exact test (see about) to calculate the enrichment at each of these ‘levels’ - although I usually choose to look at the integrated data summarized under the ‘GOTERM_[BP/CC/MF]_DIRECT’.\nclick on the + next too the &lt; Gene_Ontology &gt; category\nSelect &lt; Chart &gt; right of the GOTERM_BP_DIRECT\n BP Enrichment\nNote the P-value and benjamini correct p-value displaying the like hood that those specific Go terms are represented by random - i.e. lost the P-value the less likely the representation of the term is a random select , therefore the more likely the term is enriched.\nTo see the list of gene involved in for instance ‘positive regulation of osteoblast differentiation’ click on the blue bar under the ‘Genes’ column.\nThis is the list you will see:\n positive regulation of osteoblast differentiation\nNow review enrichment for Cell Component and Molecular Function."
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#pathway-analysis",
    "href": "5.4_DEG_Functional_Interpretation.html#pathway-analysis",
    "title": "DEG Functional Interpretation",
    "section": "4. Pathway analysis",
    "text": "4. Pathway analysis\nclick on the + next too the &lt; Pathways &gt; category\n Pathway Options\nSelect &lt; Chart &gt; right of the KEGG\n KEGG Enrichment\nYou will see a list of enriched pathways - Note the P-value and benjamini correct p-value displaying the like hood that those specific pathways are represented by random - i.e. lost the P-value the less likely the representation of the pathway is a random select, therefore the more likely the pathways is enriched.\nclick on the &lt; TNF Signalling Pathway &gt;_ in the term column\nThis will display the pathway with the terms that are present in the list shown with red stars or highlight by redtext below the figure.\n TNF Signalling Pathway\nExplore some more pathways"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#functional-annotation-clustering",
    "href": "5.4_DEG_Functional_Interpretation.html#functional-annotation-clustering",
    "title": "DEG Functional Interpretation",
    "section": "5. Functional Annotation Clustering",
    "text": "5. Functional Annotation Clustering\nDAVID attempts to summarise enrichment between categorization systems using a tool it terms - Functional annotation clustering. If you select this for you current data set. you will be provided with the following clusters:\n Functional Annotation Clustering\nEach cluster is assigned an enrichment score under which ‘terms’ that are enriched under different classification systems are displayed grouped together. Each ‘grouping’ is given a ‘Enrichment Score’ the larger the enrichment score the higher the score the more likely that cluster is being enriched. Note that the is a ‘Classification Stringency’ pull down menu that allows you to define the strength of associated of the term being grouped together.\nFor annotation Cluster 6 (which contains lots of Metallothionein/cadimum associated terms) there is a small green and black box - after the Enrichment Score and the ‘G’ - click on this box. This brings up a cluster matrix showing the gene gene products on the Y-axis and the classification ‘vlasses’ on the X-axis.\n Cluster Matrix\nDavid Workshop\n\nEnrichment with David Panopto"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#string-panopto",
    "href": "5.4_DEG_Functional_Interpretation.html#string-panopto",
    "title": "DEG Functional Interpretation",
    "section": "STRING Panopto",
    "text": "STRING Panopto"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#export-go-terms-and-p-values",
    "href": "5.4_DEG_Functional_Interpretation.html#export-go-terms-and-p-values",
    "title": "DEG Functional Interpretation",
    "section": "1. Export GO terms and P-values",
    "text": "1. Export GO terms and P-values\n\n1A. gprofiler\nUnder the detailed results menu there is a CSV icon which when clicked can be used to export your enriched terms as a CSV which can then be imported into excel. You can use the associated setting button (the cog symbol next toi the CSV) to select only GO terms to export for this exercise.\n gprofiler export\nYou will then need to copy column C and D into Revigo\n\n\n1B. DAVID\nAfter you have completed the functional analysis select &lt; Gene_ontology &gt; GOTERM_BP_DIRECT &gt; Chart &gt;\nnow Select &lt; Download File &gt;\n DAVID Go Download\nThis will either display the enrichment table into the browser window or ask for a save location depending on your browser settings. If you are not asked for a save location right hand click the displayed table and select Save as and save to a appropriate location as a text file. This text file can be opened/imported into excel. You will file the GO term is concatenated with the GO description in Column B. To split this insert a blank column after column B, then select column B and select menu option &lt; Data &gt; Test to columns &gt;. Select &lt; Delimited &gt; Next &gt;_ and use the radio button to select other and add a ~_ to the box directly to the right of this option. Now click &lt; Next and Finish &gt;. You will see the GO term in now in Column B and the P-value in column F. Select these columns and paste them into Revigo.\nYou should now repeat this process for Molecular Function and Cell Component Terms.When all the data is merged you can paste the GO term and P value into Revigo."
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#revigo-data-entry",
    "href": "5.4_DEG_Functional_Interpretation.html#revigo-data-entry",
    "title": "DEG Functional Interpretation",
    "section": "2. Revigo data entry",
    "text": "2. Revigo data entry\nEnter GO terms and P-values into Revigo - they should be in format\nTerm    PValue\nGO:0045892  7.75E-06\nGO:0006694  3.29E-04\nLeave the default settings and select &lt; Start Revigo &gt;"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#review-revigo-results",
    "href": "5.4_DEG_Functional_Interpretation.html#review-revigo-results",
    "title": "DEG Functional Interpretation",
    "section": "3. Review Revigo Results",
    "text": "3. Review Revigo Results\nFor each GO term category Revigo will generate a Scatterplot, Table, 3D scatter plot, interactive Graph and Tree Map. Note: at the bottom of each plot there are export options for R and other formats _.\nThe most intuitive format is are the Tree Maps (see below) whilst the Scatterplot output table, which include terms like ‘Frequency and Uniqueness’ can be used to create networks in Cytoscape.\n Revigo BP TreeMap"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#revigo-workshop",
    "href": "5.4_DEG_Functional_Interpretation.html#revigo-workshop",
    "title": "DEG Functional Interpretation",
    "section": "Revigo Workshop",
    "text": "Revigo Workshop"
  },
  {
    "objectID": "5.4_DEG_Functional_Interpretation.html#revigo-panopto",
    "href": "5.4_DEG_Functional_Interpretation.html#revigo-panopto",
    "title": "DEG Functional Interpretation",
    "section": "Revigo Panopto",
    "text": "Revigo Panopto"
  },
  {
    "objectID": "6.1_metabarcoding.html",
    "href": "6.1_metabarcoding.html",
    "title": "Metagenomics",
    "section": "",
    "text": "Metabarcoding Powerpoint\nThis workshop is based heavily on QIIME’s own tutorial, Moving Pictures of the Human Microbiome. I have just modified this tutorial slightly to tailor it to our context and computer system."
  },
  {
    "objectID": "6.1_metabarcoding.html#importing-data-in-qiime",
    "href": "6.1_metabarcoding.html#importing-data-in-qiime",
    "title": "Metagenomics",
    "section": "Importing data in QIIME",
    "text": "Importing data in QIIME\nQIIME handles data by importing it into its own file format, called a QIIME artefact. This contains not only the data itself but a record of all the processes that it has gone through. Start by loading up QIIME.\n\nmodule load qiime2/2022.8 \n\nThe first step in the analysis is to read the data in.\nYou will see that the import command below has backslashes at the end of each line. This isn’t specific to QIIME, but a universal way to break up a complex command into multiple lines so that it’s easier to read. In Linux, a backslash is called an escape character; it means “interpret the next character literally”. Normally pressing enter at the end of a command tells the computer to run that command. Typing a backslash before pressing enter tells the computer that you literally just want a line break.\nLet’s explore the parts of the command\n\nqiime tools import is the QIIME programme to be used\ntype EMPSingleEndSequences tells QIIME the format the sequences will be in\ninput-path emp-single-end-sequences gives the name of the directory the sequence files are in\noutput-path emp-single-end-sequences.qza gives the name of the artefact file to be created. Note the file extension: .qza\n\n\nqiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path emp-single-end-sequences \\\n  --output-path emp-single-end-sequences.qza\n\nHenceforth, I have removed some parts of the script! Every time you see YOURFILE, it’s up to you to replace it with the correct file name."
  },
  {
    "objectID": "6.1_metabarcoding.html#demultiplexing",
    "href": "6.1_metabarcoding.html#demultiplexing",
    "title": "Metagenomics",
    "section": "Demultiplexing",
    "text": "Demultiplexing\nAs mentioned above, this step would normally be done automatically be the sequencing centre. However, it’s good to have a go at demultiplexing as there are still occasions when it needs to be done manually.\nThis command takes the QIIME artefact we have just created and uses the barcode information to decide which sample each sequence belongs to. It creates another QIIME artefact containing the demultiplexed samples.\n\nqiime demux emp-single \\\n  --i-seqs YOURFILE \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column barcode-sequence \\\n  --o-per-sample-sequences demux.qza \\\n  --o-error-correction-details demux-details.qza\n\nWe can then ask QIIME to produce some summary statistics. The output here is a .qzv: this is a QIIME visualisation file. Download a qzv and then drag and drop it into view.qiime2.org/\n\nqiime demux summarize \\\n  --i-data demux.qza \\\n  --o-visualization demux.qzv\n\nHave a look at the qzv file. What does it tell you about the data?"
  },
  {
    "objectID": "6.1_metabarcoding.html#quality-filtering-and-counting",
    "href": "6.1_metabarcoding.html#quality-filtering-and-counting",
    "title": "Metagenomics",
    "section": "Quality filtering and counting",
    "text": "Quality filtering and counting\nThe next step in the process does two things at once. Firstly, it quality checks the data. It removes any PhiX reads leftover from the sequencing, checks for low quality reads, and removes chimeras (hybrid reads created when two PCR products get erroneously stuck together). Secondly, it counts how many times each unique sequence (ASV) occurs in each sample.\nQIIME offers a few different pipelines for this step, but we are going to use one called DADA2. This is the most computationally intensive step, so be prepared for it to take up to 10 minutes to complete.\nHave a look at the help page for this command at https://docs.qiime2.org/2022.8/plugins/available/dada2/denoise-single/\n\nWhat are the p-trim-left and p-trunc-len options doing? Would there be a better way to have dealt with this problem in the data? HINT: it would need to be done before reading into QIIME!\n\n\nqiime dada2 denoise-single \\\n  --i-demultiplexed-seqs YOURFILE \\\n  --p-trim-left 0 \\\n  --p-trunc-len 120 \\\n  --o-representative-sequences rep-seqs.qza \\\n  --o-table table.qza \\\n  --o-denoising-stats stats.qza\n\nThis command produces three outputs. One is the table (how many reads per ASV per sample). Another is the representative sequences: for each ASV, it marries up the identifier with the actual sequence. Finally, there are some stats on the process.\nHaving produced our outputs, we can run some summaries to see how it’s gone.\n\nqiime feature-table summarize \\\n  --i-table YOURFILE \\\n  --o-visualization table.qzv \\\n  --m-sample-metadata-file YOURFILE\n  \nqiime feature-table tabulate-seqs \\\n  --i-data YOURFILE \\\n  --o-visualization rep-seqs.qzv\n\nLook at these qzv files. What information does each give you?"
  },
  {
    "objectID": "6.1_metabarcoding.html#taxonomy-assignment",
    "href": "6.1_metabarcoding.html#taxonomy-assignment",
    "title": "Metagenomics",
    "section": "Taxonomy assignment",
    "text": "Taxonomy assignment\nHaving counted the number of times each sequence occurs in each sample, we really want to know what organism that sequence came from. QIIME uses a machine learning tool (a Naive Bayes classifier, if you’re into that kind of thing) to assign a taxonomic identity to each sequence. The classifier is trained by giving it a database of sequences of known identity. This approach is endlessly flexible, as you can train the classifier to any type of sequence you are interested in. However, we will be using a pre-trained classifier as our data relates to a commonly-used 16S rRNA region. This classifier has been trained on the Silva database of 16S rRNA genes, focussing in just on the 515-806 region targeted by our primers.\n\nqiime feature-classifier classify-sklearn \\\n  --i-classifier silva-138-99-515-806-nb-classifier.qza\\\n  --i-reads rep-seqs.qza \\\n  --o-classification taxonomy.qza\n\nqiime metadata tabulate \\\n  --m-input-file taxonomy.qza \\\n  --o-visualization taxonomy.qzv"
  },
  {
    "objectID": "6.1_metabarcoding.html#barplots",
    "href": "6.1_metabarcoding.html#barplots",
    "title": "Metagenomics",
    "section": "Barplots",
    "text": "Barplots\nNow we can get QIIME to make its famous barplots! You will see these in many, many microbiome papers.\n\nqiime taxa barplot \\\n  --i-table YOURFILE \\\n  --i-taxonomy YOURFILE \\\n  --m-metadata-file YOURFILE \\\n  --o-visualization taxa-bar-plots.qzv\n\nDownload the qzv and have a play with the options!"
  },
  {
    "objectID": "6.1_metabarcoding.html#diversity-exploration",
    "href": "6.1_metabarcoding.html#diversity-exploration",
    "title": "Metagenomics",
    "section": "Diversity exploration",
    "text": "Diversity exploration\nQIIME will also compute a lot of other statistics on your data. I personally prefer to do this in R, but for the sake of completeness let’s also look at the QIIME output (I admit, some of it is quite pretty).\n\nqiime diversity core-metrics \\\n  --i-table YOURFILE \\\n  --p-sampling-depth 1103 \\\n  --m-metadata-file YOURFILE \\\n  --output-dir core-metrics-results\n\n\nqiime emperor plot \\\n  --i-pcoa core-metrics-results/bray_curtis_pcoa_results.qza \\\n  --m-metadata-file sample-metadata.tsv \\\n  --p-custom-axes days-since-experiment-start \\\n  --o-visualization core-metrics-results/bray-curtis-emperor-days-since-experiment-start.qzv"
  },
  {
    "objectID": "6.1_metabarcoding.html#references",
    "href": "6.1_metabarcoding.html#references",
    "title": "Metagenomics",
    "section": "References",
    "text": "References\nBokulich NA, Kaehler BD, Rideout JR, et al. Optimizing taxonomic classification of marker‐gene amplicon sequences with QIIME 2’s q2‐feature‐classifier plug. Microbiome. 2018a;6:90\nBolyen E, Rideout JR, Dillon MR, et al. 2019. Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2. Nature Biotechnology 37: 852–857. https://doi.org/10.1038/s41587-019-0209-9\nCallahan BJ, McMurdie PJ, Rosen MJ, et al. DADA2: high‐resolution sample inference from Illumina amplicon data. Nature Methods 2016;13:581‐583.\nCaporaso JG, Lauber CL, Costello EK, Berg-Lyons D, Gonzalez A, Stombaugh J, Knights D, Gajer P, Ravel J, Fierer N, Gordon JI, Knight R. Moving pictures of the human microbiome. Genome Biology 2011;12(5):R50. doi: 10.1186/gb-2011-12-5-r50. PMID: 21624126; PMCID: PMC3271711.\nCaporaso, J. G., Lauber, C. L., Walters, W. A., Berg-Lyons, D., Lozupone, C. A., Turnbaugh, P. J., Noah Fierer, N., & Knight, R. (2011). Global patterns of 16S rRNA diversity at a depth of millions of sequences per sample. Proceedings of the Natural Academy of Sciences USA 108, 4516–4522. http://doi.org/10.1073/pnas.1000080107\nCaporaso, J. G., Lauber, C. L., Walters, W. A., Berg-Lyons, D., Huntley, J., Fierer, N., Owens, S. M., Betley, J., Fraser, L., Bauer, M., Gormley, N., Gilbert, J. A., Smith, G., & Knight, R. (2012). Ultra-high-throughput microbial community analysis on the Illumina HiSeq and MiSeq platforms. ISME Journal 6, 1621–1624. http://doi.org/10.1038/ismej.2012.8"
  },
  {
    "objectID": "6.1_metabarcoding.html#workshop-run-through",
    "href": "6.1_metabarcoding.html#workshop-run-through",
    "title": "Metagenomics",
    "section": "Workshop Run-Through",
    "text": "Workshop Run-Through"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html",
    "href": "Commandline_Tools_and_Scripting.html",
    "title": "Commandline Tools and Scripting",
    "section": "",
    "text": "{bash include=FALSE} knitr::opts_chunk$set(eval = FALSE)"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "href": "Commandline_Tools_and_Scripting.html#use-zgrep-with-compressed-files",
    "title": "Commandline Tools and Scripting",
    "section": "Use Zgrep with compressed files",
    "text": "Use Zgrep with compressed files\nYou can use all of grep functionality with a compressed file by adding a ‘z’ - just replace grep with zprep and you can directly query that .gz files."
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#text-files-word-processors-and-bioinformatics",
    "href": "Commandline_Tools_and_Scripting.html#text-files-word-processors-and-bioinformatics",
    "title": "Commandline Tools and Scripting",
    "section": "Text files, Word Processors and Bioinformatics",
    "text": "Text files, Word Processors and Bioinformatics\nDocuments written using a word processor such as Microsoft Word or OpenOffice Write are not plain text documents. If your filename has an extension such as .doc or .odt, it is unlikely to be a plain text document. (Try opening a Word document in notepad or another text editor on Windows if you want proof of this.)\nWord processors are very useful for preparing documents, but we recommend you do not use them for working with bioinformatics-related files.\nWe recommend that you prepare text files for bioinformatics analyses using Linux-based text editors and not Windows- or Mac-based text editors. This is because Windows- or Mac-based text editors may insert hidden characters that are not handled properly by Linux-based programs.\nThere are a number of different text editors available on Bio-Linux. These range in ease of use, and each has its pros and cons. In this practical we will briefly look at two editors, nano and vi. ## Nano\nPros:\nvery easy – For example, command options are visible at the bottom of the window can be used when logged in without graphical support fast to start up and use\nCons:\nby default it puts return characters into lines too long for the screen (i.e. using nano for system administration can be dangerous!) This behavior can be changed by setting different defaults for the program or running it with the –w option. It is not completely intuitive for people who are used to graphical word processors"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#vi-or-vim",
    "href": "Commandline_Tools_and_Scripting.html#vi-or-vim",
    "title": "Commandline Tools and Scripting",
    "section": "Vi (or Vim)",
    "text": "Vi (or Vim)\nPros:\nAppears on nearly every Unix system. Can be very powerful if you take the time to know the key-short cuts.\nCons:\nYou have to know the shortcuts!! There’s no menus and no on screen prompts\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a file with nano\n```{bash}\nnano test_nano.txt \n```\ntype some text, exit ctrl X, save and return to command line now list the contents of the file you created\n```{bash}\nless test_nano.txt \n```\n\n\nCreate a file with vi\nvi test_vi.txt \ntype ‘a’ and you can then add text\nexit saving you edits [esc]:wq! - this stands for write quit !!\nnow list the contents of the file you created\n```{bash}\nless test_vi.txt \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "href": "Commandline_Tools_and_Scripting.html#step-1-create-your-script",
    "title": "Commandline Tools and Scripting",
    "section": "Step 1: Create your Script",
    "text": "Step 1: Create your Script\nMake a text file containing the script in question. This can be achieved by downloading or transferring the scripts as a file in the correct format. Sometimes the scripts are posted as part of a website such as a web-post in a discussion forum. To use these scripts, create a file using vi or nano and copy into the test file the script in question ensuring you save it with an appropriate name.\nHere’s a script you can try\n```{bash}\n#!/bin/bash \n\necho {10..1} \n\necho 'Blast off' \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "href": "Commandline_Tools_and_Scripting.html#step-2-make-your-script-executable",
    "title": "Commandline Tools and Scripting",
    "section": "Step 2: Make your script executable",
    "text": "Step 2: Make your script executable\nMake the file executable. Before you can run the script you must make it executable. This is done by changing its property using\n```{bash}\nchmod a+x [script name] \n```\nthis is shorthand for chmod (change modify) a(all)+(add)execute(e) [script name] – thus changing the permission to allow everyone to execute a script. For more guide to chmod see https://en.wikipedia.org/wiki/Chmod"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "href": "Commandline_Tools_and_Scripting.html#step-3-run-your-script",
    "title": "Commandline Tools and Scripting",
    "section": "Step 3: Run Your Script",
    "text": "Step 3: Run Your Script\nRun the program. This should be easy but there are a few ways of doing this.\nPlace the program into the directory where you want to use it and type\n```{bash}\n./[script name] parameters arguments \n```\nOn first use try to run with no parameters or arguments or with -h and -help to see the manual for the script. Some poorly written scripts will need you to define the program you need to use them. Ie if it was a perl program (you may have to module load perl before you run this example).\n```{bash}\nperl [script name] parameters arguments \n```\nRun from scripts current location using full path.\n```{bash}\n/full path/[script name] parameters arguments \n```\nPlace the script into your ‘PATH’ – this means that the computer automatically knows about the script and will run it from any location just given the program name. I suggest that if you want to do this ask the demonstrators and they can show you……this is advanced as if you put two scripts with the same name into the PATH you can cause issues."
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-numerical-variables",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using numerical variables",
    "text": "Loops using numerical variables\nCreating a Loop\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nfor i in {1..[number]}; do \n\n# use hash to include some level of documentation so when you get to script in a few months time \n\n# you can remember what it was all about.  ${i} = number which increment by 1 each time the loop runs\n\n[your commands]${i} \n\ndone\n```\nsave.\nnow make the program executable\n```{bash}\nchmod +x [program_name]\n```\nrun\n```{bash}\n./[program_name]\n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-strings-lists-as-variables",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using Strings (lists) as variables",
    "text": "Loops using Strings (lists) as variables\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nfor i in sampleA sampleB sampleC sampleD; do \n\n# use hash to include some level of documentation so when you get to script in a few months time\n\n# you can remember what it was all about.  ${i} = the list of strings given at the start of the for loop \n\n[your commands]${i} \n\ndone \n```\nsave\nnow make the program executable\n```{bash}\nchmod +x [program_name] \n```\nrun\n```{bash}\n./[program_name] \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "href": "Commandline_Tools_and_Scripting.html#loops-using-directory-listings",
    "title": "Commandline Tools and Scripting",
    "section": "Loops using directory listings",
    "text": "Loops using directory listings\nThis is a great method if you want to execute a series of command on a set of data files contained in a specific directory, for instance a series of sequence files.\nInvoke a text editor such as nano, then type\n```{bash}\n#!/bin/bash\n\nsequence_dir=[location of folder containing paired end sequence files]\n#*_R1.fastq - lists all files ending in _R1.fastq\n\nfor f in ${sequence_dir}/*_R1.fastq\ndo\n\n#the file name are placed in variable $f - we can separate the name of the file away from the suffix (.fastq) using this simple cut expression - the variable 'R1' now contains the file name with no suffix \nR1=$(basename $f | cut -f1 -d.)\n\n#Sometimes we want the 'base' name of the file without the direction suffix (_R1) - this expression creates a variable 'base' where the _R1 has been replace is nothing - ie removed \nbase=$(echo $R1 | sed 's/_R1//')\n\necho ${base}\n\ndone \n```\nsave\nnow make the program executable\n```{bash}\nchmod +x [program_name] \n```\nrun\n```{bash}\n./[program_name] \n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#gz-files",
    "href": "Commandline_Tools_and_Scripting.html#gz-files",
    "title": "Commandline Tools and Scripting",
    "section": "gz files",
    "text": "gz files\n….wait a minute do you really need to decompress this file !! Many programs will happily use a .gz file directly, this a win for your file space so check out if you really need to decompress the file. Unfortunate some utilits like ‘sed’ require files to be unzipped, it that case:\n```{bash}\n#to decompress\ngunzip [filename].gz\n#to recompress\ngzip [filename]\n```"
  },
  {
    "objectID": "Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "href": "Commandline_Tools_and_Scripting.html#how-to-extract-a-.tar.gz-file-on-linux",
    "title": "Commandline Tools and Scripting",
    "section": "How to extract a .tar.gz file on Linux?",
    "text": "How to extract a .tar.gz file on Linux?\nTo extract a .tar.gz file on Linux, you can use the “tar” command in the terminal. Here is the general syntax:\n```{bash}\ntar -xvzf filename.tar.gz\n```\nHere is a brief explanation of the options used:\n```{bash}\n-x: This option tells tar to extract the contents of the archive.\n\n-v: This option is for verbose output, which means that tar will display a list of the files being extracted as it does so.\n\n-z: This option tells tar to decompress the archive using gzip.\n\n-f: This option is used to specify the archive file to extract.\n```"
  }
]